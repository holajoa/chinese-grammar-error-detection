{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from wrapper import *\n",
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\holaj\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.633 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', sep='\\t')\n",
    "\n",
    "model_name = 'hfl/chinese-macbert-base'\n",
    "ner_model_name = 'uer/roberta-base-finetuned-cluener2020-chinese'\n",
    "\n",
    "train_dataset_config = {\n",
    "    'model_name':model_name,\n",
    "    'aux_model_name':ner_model_name,\n",
    "    'maxlength':64,\n",
    "    'train_val_split':-1,\n",
    "    'test':False, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "}\n",
    "\n",
    "train = DatasetWithAuxiliaryEmbeddings(df=train_df, **train_dataset_config)\n",
    "train.tokenize()\n",
    "train.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45\n",
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-address\",\n",
      "    \"2\": \"I-address\",\n",
      "    \"3\": \"B-book\",\n",
      "    \"4\": \"I-book\",\n",
      "    \"5\": \"B-company\",\n",
      "    \"6\": \"I-company\",\n",
      "    \"7\": \"B-game\",\n",
      "    \"8\": \"I-game\",\n",
      "    \"9\": \"B-government\",\n",
      "    \"10\": \"I-government\",\n",
      "    \"11\": \"B-movie\",\n",
      "    \"12\": \"I-movie\",\n",
      "    \"13\": \"B-name\",\n",
      "    \"14\": \"I-name\",\n",
      "    \"15\": \"B-organization\",\n",
      "    \"16\": \"I-organization\",\n",
      "    \"17\": \"B-position\",\n",
      "    \"18\": \"I-position\",\n",
      "    \"19\": \"B-scene\",\n",
      "    \"20\": \"I-scene\",\n",
      "    \"21\": \"S-address\",\n",
      "    \"22\": \"S-book\",\n",
      "    \"23\": \"S-company\",\n",
      "    \"24\": \"S-game\",\n",
      "    \"25\": \"S-government\",\n",
      "    \"26\": \"S-movie\",\n",
      "    \"27\": \"S-name\",\n",
      "    \"28\": \"S-organization\",\n",
      "    \"29\": \"S-position\",\n",
      "    \"30\": \"S-scene\",\n",
      "    \"31\": \"[PAD]\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-address\": 1,\n",
      "    \"B-book\": 3,\n",
      "    \"B-company\": 5,\n",
      "    \"B-game\": 7,\n",
      "    \"B-government\": 9,\n",
      "    \"B-movie\": 11,\n",
      "    \"B-name\": 13,\n",
      "    \"B-organization\": 15,\n",
      "    \"B-position\": 17,\n",
      "    \"B-scene\": 19,\n",
      "    \"I-address\": 2,\n",
      "    \"I-book\": 4,\n",
      "    \"I-company\": 6,\n",
      "    \"I-game\": 8,\n",
      "    \"I-government\": 10,\n",
      "    \"I-movie\": 12,\n",
      "    \"I-name\": 14,\n",
      "    \"I-organization\": 16,\n",
      "    \"I-position\": 18,\n",
      "    \"I-scene\": 20,\n",
      "    \"O\": 0,\n",
      "    \"S-address\": 21,\n",
      "    \"S-book\": 22,\n",
      "    \"S-company\": 23,\n",
      "    \"S-game\": 24,\n",
      "    \"S-government\": 25,\n",
      "    \"S-movie\": 26,\n",
      "    \"S-name\": 27,\n",
      "    \"S-organization\": 28,\n",
      "    \"S-position\": 29,\n",
      "    \"S-scene\": 30,\n",
      "    \"[PAD]\": 31\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8\n",
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertWithNER(bert_model=model_name, ner_model=ner_model_name)\n",
    "model.load_state_dict(torch.load('../ner_run_v2/fold0/checkpoint-4950/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = AdversarialTrainingArguments(\n",
    "    output_dir='./eval', \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16, \n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save checkpoint at each epoch\n",
    "    learning_rate=2e-5, \n",
    "    load_best_model_at_end=True,\n",
    "    epsilon=0, \n",
    "    alpha=.3, \n",
    "    gamma=1, \n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(\n",
    "    model=model, \n",
    "    args=arguments, \n",
    "    eval_dataset=train.dataset['train'], \n",
    "    tokenizer=train.tokenizer, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids, labels. If token_type_ids, labels are not expected by `BertWithNER.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 45248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\eval-ner.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/eval-ner.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prediction_output \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(train\u001b[39m.\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:2832\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2829\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2831\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2832\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2833\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[0;32m   2834\u001b[0m )\n\u001b[0;32m   2835\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2836\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2837\u001b[0m     speed_metrics(\n\u001b[0;32m   2838\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2842\u001b[0m     )\n\u001b[0;32m   2843\u001b[0m )\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:2936\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2933\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[0;32m   2935\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 2936\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[0;32m   2937\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2939\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:3177\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   3175\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[0;32m   3176\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3177\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   3178\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m   3180\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\wrapper.py:90\u001b[0m, in \u001b[0;36mAdversarialTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mHow the loss is computed by Trainer. By default, all models return the loss in the first element.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mSubclass and override for custom behavior.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m---> 90\u001b[0m true_labels \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(outputs):\n\u001b[0;32m     92\u001b[0m     logits \u001b[39m=\u001b[39m outputs\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:236\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39metc.).\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39mIf the key is an integer, get the `tokenizers.Encoding` for batch item with index `key`.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m    237\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "prediction_output = trainer.predict(train.dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "eval_df = copy(train_df)\n",
    "eval_df['prediction'] = np.argmax(prediction_output.predictions, 1)\n",
    "eval_df = eval_df[['id', 'label', 'prediction', 'text']]\n",
    "\n",
    "mislabelled_df = eval_df[eval_df['label'] != eval_df['prediction']]\n",
    "mislabelled_df.to_csv('mislabelled-mini-with-ner.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
