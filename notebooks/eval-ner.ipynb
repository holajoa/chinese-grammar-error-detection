{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from wrapper import *\n",
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\holaj\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.633 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', sep='\\t')\n",
    "\n",
    "model_name = 'hfl/chinese-macbert-base'\n",
    "ner_model_name = 'uer/roberta-base-finetuned-cluener2020-chinese'\n",
    "\n",
    "train_dataset_config = {\n",
    "    'model_name':model_name,\n",
    "    'aux_model_name':ner_model_name,\n",
    "    'maxlength':64,\n",
    "    'train_val_split':-1,\n",
    "    'test':False, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "}\n",
    "\n",
    "train = DatasetWithAuxiliaryEmbeddings(df=train_df, **train_dataset_config)\n",
    "train.tokenize()\n",
    "train.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertWithNER(bert_model=model_name, ner_model=ner_model_name)\n",
    "model.load_state_dict(torch.load('../ner_run_v2/fold0/checkpoint-4950/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = AdversarialTrainingArguments(\n",
    "    output_dir='./eval', \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16, \n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save checkpoint at each epoch\n",
    "    learning_rate=1e-5, \n",
    "    load_best_model_at_end=True,\n",
    "    epsilon=0, \n",
    "    alpha=.3, \n",
    "    gamma=1, \n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(\n",
    "    model=model, \n",
    "    args=arguments, \n",
    "    eval_dataset=train.dataset['train'], \n",
    "    tokenizer=train.tokenizer, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids, labels. If token_type_ids, labels are not expected by `BertWithNER.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 45248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\eval-ner.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/eval-ner.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prediction_output \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(train\u001b[39m.\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:2832\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2829\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2831\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2832\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2833\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[0;32m   2834\u001b[0m )\n\u001b[0;32m   2835\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2836\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2837\u001b[0m     speed_metrics(\n\u001b[0;32m   2838\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2842\u001b[0m     )\n\u001b[0;32m   2843\u001b[0m )\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:2936\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2933\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[0;32m   2935\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 2936\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[0;32m   2937\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2939\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\trainer.py:3177\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   3175\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[0;32m   3176\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3177\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   3178\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m   3180\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\wrapper.py:90\u001b[0m, in \u001b[0;36mAdversarialTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mHow the loss is computed by Trainer. By default, all models return the loss in the first element.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mSubclass and override for custom behavior.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m---> 90\u001b[0m true_labels \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(outputs):\n\u001b[0;32m     92\u001b[0m     logits \u001b[39m=\u001b[39m outputs\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:236\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39metc.).\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39mIf the key is an integer, get the `tokenizers.Encoding` for batch item with index `key`.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m    237\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "prediction_output = trainer.predict(train.dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "eval_df = copy(train_df)\n",
    "eval_df['prediction'] = np.argmax(prediction_output.predictions, 1)\n",
    "eval_df = eval_df[['id', 'label', 'prediction', 'text']]\n",
    "\n",
    "mislabelled_df = eval_df[eval_df['label'] != eval_df['prediction']]\n",
    "mislabelled_df.to_csv('mislabelled-mini-with-ner.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
