{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from preprocess import *\n",
    "from wrapper import *\n",
    "from models import *\n",
    "\n",
    "torch.cuda.is_available()\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data-org/train.csv', sep='\\t').set_index('id')\n",
    "corpus = df[df.label == 0].drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "ds = SimpleDataset(\n",
    "    corpus, \n",
    "    model_name='KoichiYasuoka/chinese-bert-wwm-ext-upos', \n",
    "    test=True, \n",
    "    eda=False, \n",
    "    device=device, \n",
    ")\n",
    "ds.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KoichiYasuoka/chinese-bert-wwm-ext-upos\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"KoichiYasuoka/chinese-bert-wwm-ext-upos\")\n",
    "\n",
    "def feedforward(\n",
    "    self, \n",
    "    ds:DatasetWithAuxiliaryEmbeddings, \n",
    "    checkpoints:List[str], \n",
    "    device:torch.device, \n",
    "    raw_outputs:bool=True, \n",
    "    output_probabilities:bool=False, \n",
    "    majority_vote=False, \n",
    ") -> np.ndarray:\n",
    "    output_tensors = []\n",
    "    output_sequence_logits = []\n",
    "\n",
    "    if 'cuda' in device.type:\n",
    "        self.model.cuda()\n",
    "\n",
    "    logits = []\n",
    "    sequence_logits = []\n",
    "    dataloader = DataLoader(ds.dataset['train'].with_format('torch'), batch_size=16)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = {k:v.to(device) for k,v in batch.items()\n",
    "                if k in ds.tokenizer.model_input_names or k == 'auxiliary_input_ids'}\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
