{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from preprocess import *\n",
    "from wrapper import *\n",
    "from models import BertWithNER, AutoModelWithNER\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', sep='\\t', index_col='id')\n",
    "test_df = pd.read_csv('../data/test.csv', sep='\\t', index_col='id')\n",
    "\n",
    "model_name = 'hfl/chinese-macbert-base'\n",
    "ner_model_name = 'uer/roberta-base-finetuned-cluener2020-chinese'\n",
    "\n",
    "test_dataset_config = {\n",
    "    'model_name':model_name,\n",
    "    'aux_model_name':ner_model_name,\n",
    "    'maxlength':128,\n",
    "    'train_val_split':-1,\n",
    "    'test':True, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'device':device,\n",
    "}\n",
    "\n",
    "train = DatasetWithAuxiliaryEmbeddings(df=test_df.reset_index(), **test_dataset_config)\n",
    "train.tokenize()\n",
    "train.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = [\n",
    "    '../ner_run/fold0/checkpoint-5092/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold1/checkpoint-7425/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold2/checkpoint-9900/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold3/checkpoint-7425/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold4/checkpoint-4950/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold5/checkpoint-9900/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold6/checkpoint-9900/pytorch_model.bin', \n",
    "    # '../ner_run_best/fold7/checkpoint-9900/pytorch_model.bin', \n",
    "]\n",
    "\n",
    "# model = BertWithNER(bert_model=model_name, ner_model=ner_model_name)\n",
    "model = AutoModelWithNER(model=model_name, ner_model=ner_model_name)\n",
    "state_dict = torch.load(checkpoints[0], map_location=device)\n",
    "# for key in list(state_dict.keys()):\n",
    "#     state_dict[key.replace('bert', 'base_model')] = state_dict.pop(key)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "output_tensors = []\n",
    "\n",
    "for cp in checkpoints:\n",
    "    # model = BertWithNER(bert_model=model_name, ner_model=ner_model_name)\n",
    "    model = AutoModelWithNER(model=model_name, ner_model=ner_model_name)\n",
    "    state_dict = torch.load(cp, map_location=device)\n",
    "    # for key in list(state_dict.keys()):\n",
    "    #     state_dict[key.replace('bert', 'base_model')] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    logits = []\n",
    "    dataloader = DataLoader(train.dataset['train'].with_format('torch'), batch_size=16)\n",
    "    for batch in dataloader:\n",
    "        outputs = model(**batch)\n",
    "        logits.append(outputs['logits'])\n",
    "\n",
    "    del model\n",
    "    output_tensors.append(torch.concat(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holaj\\AppData\\Local\\Temp\\ipykernel_29028\\1890036286.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['prediction'] = torch.argmax(output_tensors[0], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>矗立在鲁迅纪念馆前的这座铜像，是由巴金等著名作家倡议，国内外热爱鲁迅的人  士集资30万元铸成的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>关于鸟类的进化，有一种观点认为，鸟最早用４个翅膀滑翔的，后来才进化成骨骼轻巧、拍动双翼的飞行...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>消费者通过网络平台购买商品，其合法权益受到损害时，可以向销售者要求赔偿。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>要实现可持续发展，就要依靠科学技术，形成少投入、多产出的生产方式和少排放、多利用的消费模式，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>杨利伟：第一位进入太空的中国航天员。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>去年，对于石油投资者和生产者，是破天荒的一年，石油价格大幅下跌，不仅改写了石油业的历史，也打...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>这家企业将携手社会力量，共同开展“健康中国”系列活动：构建健康生态、倡导健康生活、宣传健康理念。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>作为消费者，你是否想过，正是某些畸形的消费需求刺激了商业经营对生态的污染和破坏，从这个意义上...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>孔子一生积极求索，“知其不可为而为之”，奋斗一生的他是他这句格言的真实写照。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4月17日出台的“国十条”、9月29日再次出台的“新国五条”以及近两个月各部委出台的一系列政...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>在“望子成龙”“望女成凤”思想的指导下，许多家长对孩子的学习采取高压严管的政策，而对其在生活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>有关专家指出，随着世界经济的一体化，贸易效率显得更加重要，企业的经营速度决定了企业在竞争中的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>“单独二孩”政策实施以后，新生人口必然会增长。一方面将改变“4－2－1”的家庭结构，另一方面...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>出口退税增量实行中央和地方分担后，是否会引发新的欠退税以及跨地区收购出口受限等问题，也需引起...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>大奎看着小栓那美滋滋的样子，感叹道：“还是生一个女儿好啊！”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>小说忌讳“主题鲜明”，因此小说家无时无刻在注意避开“主题鲜明”的陷阱，而在通往主题的道路上费...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>加快农田水利发展，是提高农业综合生产能力，实现农民持续增收的基本保障。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>着眼于扩大内需的积极财政政策，确实实现了繁荣经济、活跃经济、拉动经济的初衷。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>本台记者采访到的医学专家表示，新型抗生素类药的临床试验即将启动，标志着我国对急性传染病的治疗...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  prediction                                               text\n",
       "id                                                                       \n",
       "505      0           1  矗立在鲁迅纪念馆前的这座铜像，是由巴金等著名作家倡议，国内外热爱鲁迅的人  士集资30万元铸成的。\n",
       "506      1           0  关于鸟类的进化，有一种观点认为，鸟最早用４个翅膀滑翔的，后来才进化成骨骼轻巧、拍动双翼的飞行...\n",
       "513      0           1               消费者通过网络平台购买商品，其合法权益受到损害时，可以向销售者要求赔偿。\n",
       "526      0           1  要实现可持续发展，就要依靠科学技术，形成少投入、多产出的生产方式和少排放、多利用的消费模式，...\n",
       "529      0           1                                 杨利伟：第一位进入太空的中国航天员。\n",
       "534      0           1  去年，对于石油投资者和生产者，是破天荒的一年，石油价格大幅下跌，不仅改写了石油业的历史，也打...\n",
       "537      1           0   这家企业将携手社会力量，共同开展“健康中国”系列活动：构建健康生态、倡导健康生活、宣传健康理念。\n",
       "539      0           1  作为消费者，你是否想过，正是某些畸形的消费需求刺激了商业经营对生态的污染和破坏，从这个意义上...\n",
       "547      1           0             孔子一生积极求索，“知其不可为而为之”，奋斗一生的他是他这句格言的真实写照。\n",
       "549      1           0  4月17日出台的“国十条”、9月29日再次出台的“新国五条”以及近两个月各部委出台的一系列政...\n",
       "550      1           0  在“望子成龙”“望女成凤”思想的指导下，许多家长对孩子的学习采取高压严管的政策，而对其在生活...\n",
       "561      0           1  有关专家指出，随着世界经济的一体化，贸易效率显得更加重要，企业的经营速度决定了企业在竞争中的...\n",
       "564      0           1  “单独二孩”政策实施以后，新生人口必然会增长。一方面将改变“4－2－1”的家庭结构，另一方面...\n",
       "567      0           1  出口退税增量实行中央和地方分担后，是否会引发新的欠退税以及跨地区收购出口受限等问题，也需引起...\n",
       "573      1           0                     大奎看着小栓那美滋滋的样子，感叹道：“还是生一个女儿好啊！”\n",
       "574      1           0  小说忌讳“主题鲜明”，因此小说家无时无刻在注意避开“主题鲜明”的陷阱，而在通往主题的道路上费...\n",
       "578      0           1                加快农田水利发展，是提高农业综合生产能力，实现农民持续增收的基本保障。\n",
       "583      0           1             着眼于扩大内需的积极财政政策，确实实现了繁荣经济、活跃经济、拉动经济的初衷。\n",
       "599      1           0  本台记者采访到的医学专家表示，新型抗生素类药的临床试验即将启动，标志着我国对急性传染病的治疗..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', sep='\\t', index_col='id')\n",
    "\n",
    "data = train_df[500:600]\n",
    "data['prediction'] = torch.argmax(output_tensors[0], 1)\n",
    "data = data[['label', 'prediction', 'text']]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(data[data.label != data.prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in train.tokenizer.model_input_names or k == 'auxiliary_input_ids'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output['logits'], axis=-1)\n",
    "        loss = cross_entropy(output['logits'], batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6985e048b942ad9e17ba0cff429a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\predict.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mset_format(\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauxiliary_input_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m eval_outputs \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mdataset[\u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(forward_pass_with_label, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\arrow_dataset.py:2376\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2373\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2375\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2377\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2378\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2379\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2380\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2381\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2382\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2383\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2384\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2385\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2386\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2387\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2388\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2389\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2390\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2391\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2392\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2393\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2394\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2395\u001b[0m     )\n\u001b[0;32m   2396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2398\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\arrow_dataset.py:551\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    550\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    552\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    553\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    554\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    512\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    513\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    514\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    515\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    516\u001b[0m }\n\u001b[0;32m    517\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    519\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    520\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    453\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    454\u001b[0m             )\n\u001b[0;32m    456\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\arrow_dataset.py:2764\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2760\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2761\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   2762\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2764\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   2765\u001b[0m         batch,\n\u001b[0;32m   2766\u001b[0m         indices,\n\u001b[0;32m   2767\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2768\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   2769\u001b[0m     )\n\u001b[0;32m   2770\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   2771\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   2772\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2773\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\datasets\\arrow_dataset.py:2644\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2642\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   2643\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 2644\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   2645\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2646\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   2647\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\predict.ipynb Cell 7\u001b[0m in \u001b[0;36mforward_pass_with_label\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m inputs \u001b[39m=\u001b[39m {k:v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m train\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmodel_input_names \u001b[39mor\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mauxiliary_input_ids\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pred_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m cross_entropy(output[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/predict.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                          reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\models.py:24\u001b[0m, in \u001b[0;36mBertWithNER.forward\u001b[1;34m(self, input_ids, attention_mask, auxiliary_input_ids, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, auxiliary_input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 24\u001b[0m     logits_base \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     25\u001b[0m     logits_ner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mner(auxiliary_input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask)\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     26\u001b[0m     concatenated_vectors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat((logits_base, logits_ner), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1011\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1011\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1014\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1015\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[0;32m   1018\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1019\u001b[0m     embedding_output,\n\u001b[0;32m   1020\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:235\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    232\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[0;32m    236\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    238\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "train.dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'auxiliary_input_ids'])\n",
    "eval_outputs = train.dataset[\"val\"].map(forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'auxiliary_input_ids', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dataset.set_format('torch')\n",
    "train.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
