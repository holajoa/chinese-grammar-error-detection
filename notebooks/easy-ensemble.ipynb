{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, Dataset\n",
    "from torch import nn \n",
    "from transformers import BertModel, AutoModelForSequenceClassification, BertForTokenClassification, EarlyStoppingCallback\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from preprocess import *\n",
    "from wrapper import *\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 743)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full = pd.read_csv('../data/train.csv', sep='\\t')\n",
    "test_df_full = pd.read_csv('../data/test.csv', sep='\\t')\n",
    "\n",
    "train_df = train_df_full[:1000]\n",
    "test_df = test_df_full.iloc[np.random.randint(0, len(test_df_full), 200)]\n",
    "\n",
    "(train_df.label == 0).sum(), (train_df.label == 1).sum()  # label distribution is pretty balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "folds = easy_ensenble_generate_kfolds(1000, 5, minority_idx)\n",
    "\n",
    "MODEL_NAME = 'hfl/chinese-macbert-base'\n",
    "\n",
    "# Define training arguments\n",
    "arguments = MyTrainingArguments(\n",
    "    output_dir=\"sample_trainer\",  # output directory\n",
    "    per_device_train_batch_size=16,  # set training and eval batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,  # number of training epochs\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save checkpoint at each epoch\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=['labels'],   # need to specify this to pass the labels to the trainer\n",
    "    epsilon=0, \n",
    "    gamma=0.2,\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "test = DatasetWithAuxiliaryEmbeddings(\n",
    "    test_df, \n",
    "    model_name=MODEL_NAME, \n",
    "    aux_model_name='uer/roberta-base-finetuned-cluener2020-chinese',\n",
    "    test=True, split_words=False, \n",
    ")\n",
    "test.tokenize()\n",
    "test.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n",
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "minority_idx = np.array(train_df.index[train_df.label == 0].tolist())\n",
    "folds = easy_ensenble_generate_kfolds(L=len(train_df), k=k, minority_idx=minority_idx)\n",
    "logits = []\n",
    "\n",
    "val_idx = folds[0]\n",
    "\n",
    "train = DatasetWithAuxiliaryEmbeddings(train_df, model_name=MODEL_NAME, \n",
    "    aux_model_name='uer/roberta-base-finetuned-cluener2020-chinese', \n",
    "    train_val_split=0.8, split_words=False\n",
    ")\n",
    "train.tokenize()\n",
    "train.construct_dataset(val_idx=val_idx)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
