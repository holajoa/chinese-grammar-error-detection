{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from wrapper import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb\n",
      "loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562\n",
      "loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-cluener2020-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-address\",\n",
      "    \"2\": \"I-address\",\n",
      "    \"3\": \"B-book\",\n",
      "    \"4\": \"I-book\",\n",
      "    \"5\": \"B-company\",\n",
      "    \"6\": \"I-company\",\n",
      "    \"7\": \"B-game\",\n",
      "    \"8\": \"I-game\",\n",
      "    \"9\": \"B-government\",\n",
      "    \"10\": \"I-government\",\n",
      "    \"11\": \"B-movie\",\n",
      "    \"12\": \"I-movie\",\n",
      "    \"13\": \"B-name\",\n",
      "    \"14\": \"I-name\",\n",
      "    \"15\": \"B-organization\",\n",
      "    \"16\": \"I-organization\",\n",
      "    \"17\": \"B-position\",\n",
      "    \"18\": \"I-position\",\n",
      "    \"19\": \"B-scene\",\n",
      "    \"20\": \"I-scene\",\n",
      "    \"21\": \"S-address\",\n",
      "    \"22\": \"S-book\",\n",
      "    \"23\": \"S-company\",\n",
      "    \"24\": \"S-game\",\n",
      "    \"25\": \"S-government\",\n",
      "    \"26\": \"S-movie\",\n",
      "    \"27\": \"S-name\",\n",
      "    \"28\": \"S-organization\",\n",
      "    \"29\": \"S-position\",\n",
      "    \"30\": \"S-scene\",\n",
      "    \"31\": \"[PAD]\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-address\": 1,\n",
      "    \"B-book\": 3,\n",
      "    \"B-company\": 5,\n",
      "    \"B-game\": 7,\n",
      "    \"B-government\": 9,\n",
      "    \"B-movie\": 11,\n",
      "    \"B-name\": 13,\n",
      "    \"B-organization\": 15,\n",
      "    \"B-position\": 17,\n",
      "    \"B-scene\": 19,\n",
      "    \"I-address\": 2,\n",
      "    \"I-book\": 4,\n",
      "    \"I-company\": 6,\n",
      "    \"I-game\": 8,\n",
      "    \"I-government\": 10,\n",
      "    \"I-movie\": 12,\n",
      "    \"I-name\": 14,\n",
      "    \"I-organization\": 16,\n",
      "    \"I-position\": 18,\n",
      "    \"I-scene\": 20,\n",
      "    \"O\": 0,\n",
      "    \"S-address\": 21,\n",
      "    \"S-book\": 22,\n",
      "    \"S-company\": 23,\n",
      "    \"S-game\": 24,\n",
      "    \"S-government\": 25,\n",
      "    \"S-movie\": 26,\n",
      "    \"S-name\": 27,\n",
      "    \"S-organization\": 28,\n",
      "    \"S-position\": 29,\n",
      "    \"S-scene\": 30,\n",
      "    \"[PAD]\": 31\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99\n",
      "loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-cluener2020-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-address\",\n",
      "    \"2\": \"I-address\",\n",
      "    \"3\": \"B-book\",\n",
      "    \"4\": \"I-book\",\n",
      "    \"5\": \"B-company\",\n",
      "    \"6\": \"I-company\",\n",
      "    \"7\": \"B-game\",\n",
      "    \"8\": \"I-game\",\n",
      "    \"9\": \"B-government\",\n",
      "    \"10\": \"I-government\",\n",
      "    \"11\": \"B-movie\",\n",
      "    \"12\": \"I-movie\",\n",
      "    \"13\": \"B-name\",\n",
      "    \"14\": \"I-name\",\n",
      "    \"15\": \"B-organization\",\n",
      "    \"16\": \"I-organization\",\n",
      "    \"17\": \"B-position\",\n",
      "    \"18\": \"I-position\",\n",
      "    \"19\": \"B-scene\",\n",
      "    \"20\": \"I-scene\",\n",
      "    \"21\": \"S-address\",\n",
      "    \"22\": \"S-book\",\n",
      "    \"23\": \"S-company\",\n",
      "    \"24\": \"S-game\",\n",
      "    \"25\": \"S-government\",\n",
      "    \"26\": \"S-movie\",\n",
      "    \"27\": \"S-name\",\n",
      "    \"28\": \"S-organization\",\n",
      "    \"29\": \"S-position\",\n",
      "    \"30\": \"S-scene\",\n",
      "    \"31\": \"[PAD]\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-address\": 1,\n",
      "    \"B-book\": 3,\n",
      "    \"B-company\": 5,\n",
      "    \"B-game\": 7,\n",
      "    \"B-government\": 9,\n",
      "    \"B-movie\": 11,\n",
      "    \"B-name\": 13,\n",
      "    \"B-organization\": 15,\n",
      "    \"B-position\": 17,\n",
      "    \"B-scene\": 19,\n",
      "    \"I-address\": 2,\n",
      "    \"I-book\": 4,\n",
      "    \"I-company\": 6,\n",
      "    \"I-game\": 8,\n",
      "    \"I-government\": 10,\n",
      "    \"I-movie\": 12,\n",
      "    \"I-name\": 14,\n",
      "    \"I-organization\": 16,\n",
      "    \"I-position\": 18,\n",
      "    \"I-scene\": 20,\n",
      "    \"O\": 0,\n",
      "    \"S-address\": 21,\n",
      "    \"S-book\": 22,\n",
      "    \"S-company\": 23,\n",
      "    \"S-game\": 24,\n",
      "    \"S-government\": 25,\n",
      "    \"S-movie\": 26,\n",
      "    \"S-name\": 27,\n",
      "    \"S-organization\": 28,\n",
      "    \"S-position\": 29,\n",
      "    \"S-scene\": 30,\n",
      "    \"[PAD]\": 31\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-cluener2020-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-address\",\n",
      "    \"2\": \"I-address\",\n",
      "    \"3\": \"B-book\",\n",
      "    \"4\": \"I-book\",\n",
      "    \"5\": \"B-company\",\n",
      "    \"6\": \"I-company\",\n",
      "    \"7\": \"B-game\",\n",
      "    \"8\": \"I-game\",\n",
      "    \"9\": \"B-government\",\n",
      "    \"10\": \"I-government\",\n",
      "    \"11\": \"B-movie\",\n",
      "    \"12\": \"I-movie\",\n",
      "    \"13\": \"B-name\",\n",
      "    \"14\": \"I-name\",\n",
      "    \"15\": \"B-organization\",\n",
      "    \"16\": \"I-organization\",\n",
      "    \"17\": \"B-position\",\n",
      "    \"18\": \"I-position\",\n",
      "    \"19\": \"B-scene\",\n",
      "    \"20\": \"I-scene\",\n",
      "    \"21\": \"S-address\",\n",
      "    \"22\": \"S-book\",\n",
      "    \"23\": \"S-company\",\n",
      "    \"24\": \"S-game\",\n",
      "    \"25\": \"S-government\",\n",
      "    \"26\": \"S-movie\",\n",
      "    \"27\": \"S-name\",\n",
      "    \"28\": \"S-organization\",\n",
      "    \"29\": \"S-position\",\n",
      "    \"30\": \"S-scene\",\n",
      "    \"31\": \"[PAD]\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-address\": 1,\n",
      "    \"B-book\": 3,\n",
      "    \"B-company\": 5,\n",
      "    \"B-game\": 7,\n",
      "    \"B-government\": 9,\n",
      "    \"B-movie\": 11,\n",
      "    \"B-name\": 13,\n",
      "    \"B-organization\": 15,\n",
      "    \"B-position\": 17,\n",
      "    \"B-scene\": 19,\n",
      "    \"I-address\": 2,\n",
      "    \"I-book\": 4,\n",
      "    \"I-company\": 6,\n",
      "    \"I-game\": 8,\n",
      "    \"I-government\": 10,\n",
      "    \"I-movie\": 12,\n",
      "    \"I-name\": 14,\n",
      "    \"I-organization\": 16,\n",
      "    \"I-position\": 18,\n",
      "    \"I-scene\": 20,\n",
      "    \"O\": 0,\n",
      "    \"S-address\": 21,\n",
      "    \"S-book\": 22,\n",
      "    \"S-company\": 23,\n",
      "    \"S-game\": 24,\n",
      "    \"S-government\": 25,\n",
      "    \"S-movie\": 26,\n",
      "    \"S-name\": 27,\n",
      "    \"S-organization\": 28,\n",
      "    \"S-position\": 29,\n",
      "    \"S-scene\": 30,\n",
      "    \"[PAD]\": 31\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/roberta-base-finetuned-cluener2020-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-address\",\n",
      "    \"2\": \"I-address\",\n",
      "    \"3\": \"B-book\",\n",
      "    \"4\": \"I-book\",\n",
      "    \"5\": \"B-company\",\n",
      "    \"6\": \"I-company\",\n",
      "    \"7\": \"B-game\",\n",
      "    \"8\": \"I-game\",\n",
      "    \"9\": \"B-government\",\n",
      "    \"10\": \"I-government\",\n",
      "    \"11\": \"B-movie\",\n",
      "    \"12\": \"I-movie\",\n",
      "    \"13\": \"B-name\",\n",
      "    \"14\": \"I-name\",\n",
      "    \"15\": \"B-organization\",\n",
      "    \"16\": \"I-organization\",\n",
      "    \"17\": \"B-position\",\n",
      "    \"18\": \"I-position\",\n",
      "    \"19\": \"B-scene\",\n",
      "    \"20\": \"I-scene\",\n",
      "    \"21\": \"S-address\",\n",
      "    \"22\": \"S-book\",\n",
      "    \"23\": \"S-company\",\n",
      "    \"24\": \"S-game\",\n",
      "    \"25\": \"S-government\",\n",
      "    \"26\": \"S-movie\",\n",
      "    \"27\": \"S-name\",\n",
      "    \"28\": \"S-organization\",\n",
      "    \"29\": \"S-position\",\n",
      "    \"30\": \"S-scene\",\n",
      "    \"31\": \"[PAD]\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-address\": 1,\n",
      "    \"B-book\": 3,\n",
      "    \"B-company\": 5,\n",
      "    \"B-game\": 7,\n",
      "    \"B-government\": 9,\n",
      "    \"B-movie\": 11,\n",
      "    \"B-name\": 13,\n",
      "    \"B-organization\": 15,\n",
      "    \"B-position\": 17,\n",
      "    \"B-scene\": 19,\n",
      "    \"I-address\": 2,\n",
      "    \"I-book\": 4,\n",
      "    \"I-company\": 6,\n",
      "    \"I-game\": 8,\n",
      "    \"I-government\": 10,\n",
      "    \"I-movie\": 12,\n",
      "    \"I-name\": 14,\n",
      "    \"I-organization\": 16,\n",
      "    \"I-position\": 18,\n",
      "    \"I-scene\": 20,\n",
      "    \"O\": 0,\n",
      "    \"S-address\": 21,\n",
      "    \"S-book\": 22,\n",
      "    \"S-company\": 23,\n",
      "    \"S-game\": 24,\n",
      "    \"S-government\": 25,\n",
      "    \"S-movie\": 26,\n",
      "    \"S-name\": 27,\n",
      "    \"S-organization\": 28,\n",
      "    \"S-position\": 29,\n",
      "    \"S-scene\": 30,\n",
      "    \"[PAD]\": 31\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\\Users\\holaj/.cache\\huggingface\\transformers\\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', sep='\\t')\n",
    "\n",
    "train_dataset_config = {\n",
    "    'model_name':'hfl/chinese-macbert-base',\n",
    "    'maxlength':64,\n",
    "    'train_val_split':-1,\n",
    "    'test':False, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'ner_config':'uer/roberta-base-finetuned-cluener2020-chinese', \n",
    "}\n",
    "\n",
    "train = SimpleDataset(df=train_df, **train_dataset_config)\n",
    "train.tokenize()\n",
    "train.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../sample_run/fold3/checkpoint-20\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../sample_run/fold3/checkpoint-20\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file ../sample_run/fold3/checkpoint-20\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../sample_run/fold3/checkpoint-20.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('../sample_run/fold3/checkpoint-20')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "arguments = AdversarialTrainingArguments(\n",
    "    output_dir='./eval', \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16, \n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save checkpoint at each epoch\n",
    "    learning_rate=1e-5, \n",
    "    load_best_model_at_end=True,\n",
    "    epsilon=0, \n",
    "    alpha=.3, \n",
    "    gamma=.5, \n",
    ")\n",
    "\n",
    "trainer = AdversarialTrainer(\n",
    "    model=model, \n",
    "    args=arguments, \n",
    "    eval_dataset=train.dataset['train'], \n",
    "    tokenizer=train.tokenizer, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ner_scores. If ner_scores are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 45248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0f26d8c4ce4f909cb365ef6e6e5580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_output = trainer.predict(train.dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "eval_df = copy(train_df)\n",
    "eval_df['prediction'] = np.argmax(prediction_output.predictions, 1)\n",
    "eval_df = eval_df[['id', 'label', 'prediction', 'text']]\n",
    "\n",
    "mislabelled_df = eval_df[eval_df['label'] != eval_df['prediction']]\n",
    "mislabelled_df.to_csv('mislabelled-mini-with-ner.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
