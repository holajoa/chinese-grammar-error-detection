{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simbert不能正常使用，除非你安装：bert4keras、tensorflow ，为了安装快捷，没有默认安装.... No module named 'bert4keras'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from d:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\synonyms\\data\\vocab.txt ...\n",
      "Loading model from cache C:\\Users\\holaj\\AppData\\Local\\Temp\\jieba.u842d550ffb06c1e965781a2cb0144564.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [d:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\synonyms\\data\\vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.258 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [d:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\synonyms\\data\\stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [d:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\synonyms\\data\\words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from preprocess import *\n",
    "from wrapper import *\n",
    "from models import *\n",
    "from pipeline import PipelineGED\n",
    "\n",
    "np.random.seed(1024)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "ntf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3402547393364929"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/data-org/train.csv', sep='\\t', index_col='id')\n",
    "\n",
    "np.random.seed(297)\n",
    "(train_df.label == 0).sum() / (train_df.label == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def get_trunc_text(text):\n",
    "    sentences = [s for s in text.split('。') if s]\n",
    "    if len(sentences) > 1:\n",
    "        prompt = jieba.cut(sentences[1], cut_all=False).__next__()\n",
    "        return sentences[0] + '。' + prompt\n",
    "    subsentences = [s for s in sentences[0].split('，') if s]\n",
    "    if len(subsentences) > 1:\n",
    "        n_kept = ceil(len(subsentences) / 3)\n",
    "        kept_subsentences = '，'.join(subsentences[:n_kept])\n",
    "        prompt = jieba.cut(subsentences[n_kept], cut_all=False).__next__()\n",
    "        return kept_subsentences + '，' + prompt\n",
    "    words = list(jieba.cut(subsentences[0], cut_all=False))\n",
    "    return ''.join(words[:len(words)//3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = train_df[train_df.label == 1]\n",
    "pos_data.head(10)\n",
    "\n",
    "pos_data_trunc = pos_data.copy(deep=True).rename(columns={'text':'full_text'})\n",
    "pos_data_trunc['text'] = pos_data_trunc.full_text.map(get_trunc_text)\n",
    "\n",
    "pos_ds = DatasetWithAuxiliaryEmbeddings(pos_data_trunc, model_name='bigscience/bloom-560m', device='cuda')\n",
    "pos_ds.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤**:\n",
    "1. sample一些training data，保留原正负比例（7:3）\n",
    "2. 把正样本开头feed进generative model，直到得到eos，这些归类为扩充负样本\n",
    "3. 在原样本上和原样本+扩充负样本上分别训练模型，在unseen dev set上对比performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac4452ce8b445f8ffedc3a09d6b6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "if 'cuda' in device.type:\n",
    "    model.cuda()\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "p = 0.25\n",
    "dataloader = DataLoader(pos_ds.dataset['train'].with_format('torch'), batch_size=4)\n",
    "\n",
    "for batch in tqdm_notebook(dataloader):\n",
    "    if np.random.random() > p:\n",
    "        continue\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=64,\n",
    "            eos_token_id=420,  \n",
    "            forced_eos_token_id=420, \n",
    "            pad_token_id=tokenizer.convert_tokens_to_ids('<pad>'),\n",
    "            no_repeat_ngram_size=3, \n",
    "            repetition_penalty=1.2, \n",
    "            top_k=5, \n",
    "        )\n",
    "        generated_sentences.extend(tokenizer.batch_decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences = [s for s in generated_sentences if len(s) < 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7496"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from preprocess import *\n",
    "from wrapper import *\n",
    "from models import *\n",
    "from pipeline import PipelineGED\n",
    "\n",
    "np.random.seed(1024)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "ntf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = 'base'\n",
    "MODEL_ARCH = 'macbert+bert-wwm'\n",
    "\n",
    "np.random.seed(297)\n",
    "\n",
    "train_df = pd.read_csv('../data/data-org/train.csv', sep='\\t', index_col='id')\n",
    "test_df = pd.read_csv('../data/data-org/test.csv', sep='\\t', index_col='id')\n",
    "\n",
    "oob_model_name = None\n",
    "if MODEL_ARCH == 'ernie':\n",
    "    model_name = 'nghuyong/ernie-gram-zh'\n",
    "elif MODEL_ARCH == 'macbert':\n",
    "    model_name = 'hfl/chinese-macbert-base' if MODEL_SIZE == 'base' else 'hfl/chinese-macbert-large'\n",
    "elif MODEL_ARCH == 'roberta-word-based':\n",
    "    model_name = 'uer/roberta-base-word-chinese-cluecorpussmall'\n",
    "elif MODEL_ARCH == 'macbert+bert-wwm':\n",
    "    model_name = 'hfl/chinese-macbert-base'\n",
    "    oob_model_name = 'KoichiYasuoka/chinese-bert-wwm-ext-upos'\n",
    "else:\n",
    "    raise NotImplementedError(f'Model {MODEL_ARCH} is not implemented yet.')\n",
    "\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "test_dataset_config = {\n",
    "    'model_name':model_name,\n",
    "    'aux_model_name':oob_model_name,\n",
    "    'maxlength':max_length,\n",
    "    'train_val_split':-1,\n",
    "    'test':True, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'device':device,\n",
    "    'split_words':False, \n",
    "    'cut_all':False, \n",
    "}\n",
    "\n",
    "test = DatasetWithAuxiliaryEmbeddings(df=test_df.reset_index(), **test_dataset_config)\n",
    "test.tokenize()\n",
    "test.construct_dataset()\n",
    "\n",
    "train_dataset_config = {\n",
    "    'model_name':model_name,\n",
    "    'aux_model_name':oob_model_name,\n",
    "    'maxlength':max_length,\n",
    "    'train_val_split':-1,\n",
    "    'test':False, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'device':device,\n",
    "    'split_words':False, \n",
    "    'cut_all':False, \n",
    "}\n",
    "\n",
    "train = DatasetWithAuxiliaryEmbeddings(df=train_df.reset_index(), **train_dataset_config)\n",
    "train.tokenize()\n",
    "train.construct_dataset()\n",
    "\n",
    "ntf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at KoichiYasuoka/chinese-bert-wwm-ext-upos were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at KoichiYasuoka/chinese-bert-wwm-ext-upos and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if MODEL_ARCH == 'macbert':\n",
    "    checkpoints = [\n",
    "        '../finetuned_models/ensemble/model0/checkpoint-2400/pytorch_model.bin',\n",
    "    ]\n",
    "elif MODEL_ARCH == 'ernie':\n",
    "    checkpoints = [f'../finetuned_models/balanced_trial_ernie_gram/fold{i}/checkpoint-1910/pytorch_model.bin' for i in range(10)] \n",
    "elif MODEL_ARCH == 'roberta-word-based':\n",
    "    checkpoints = [\n",
    "        \"../finetuned_models/word-based-roberta/model0/checkpoint-1500/pytorch_model.bin\", \n",
    "    ]\n",
    "elif MODEL_ARCH == 'macbert+bert-wwm':\n",
    "    checkpoints = [\n",
    "        \"../finetuned_models/macbert_with_wwm_upos/model0/checkpoint-5832/pytorch_model.bin\", \n",
    "        \"../finetuned_models/macbert_with_wwm_upos/model1/checkpoint-6811/pytorch_model.bin\", \n",
    "        \"../finetuned_models/macbert_with_wwm_upos/model2/checkpoint-6776/pytorch_model.bin\", \n",
    "        \"../finetuned_models/macbert_with_wwm_upos/model3/checkpoint-5826/pytorch_model.bin\", \n",
    "        \"../finetuned_models/macbert_with_wwm_upos/model4/checkpoint-4860/pytorch_model.bin\", \n",
    "\n",
    "    ]\n",
    "else:\n",
    "    print(f'Model {MODEL_ARCH} is not implemented yet.')\n",
    "\n",
    "data_configs = {\n",
    "    'model_name':model_name,\n",
    "    'maxlength':max_length,\n",
    "    'train_val_split':-1,\n",
    "    'test':True, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'split_words':False, \n",
    "    'cut_all':False, \n",
    "}\n",
    "clf = PipelineGED(\n",
    "    model_name=model_name, \n",
    "    oob_model_name=oob_model_name, \n",
    "    data_configs=data_configs, \n",
    "    pooling_mode='cls',  \n",
    "    model_architecture='bert_with_oob_model', \n",
    ")\n",
    "\n",
    "def apply_ged_pipeline_oob(texts, checkpoints=checkpoints, majority_vote=False):\n",
    "    probs = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True, display=False)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:52<00:00,  8.99it/s]\n",
      "100%|██████████| 469/469 [00:53<00:00,  8.73it/s]\n",
      "100%|██████████| 469/469 [00:54<00:00,  8.65it/s]\n",
      "100%|██████████| 469/469 [00:54<00:00,  8.53it/s]\n",
      "100%|██████████| 469/469 [00:55<00:00,  8.49it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = generated_sentences\n",
    "probs = apply_ged_pipeline_oob(texts, checkpoints=checkpoints, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>为了营造和谐、安全的校园环境，确保广大未成年人的交通安全，天元区公安局交警大队在辖区范围内开...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>在翻阅中国话剧100周年纪念活动资料时，他发现《红楼梦》这部戏的演出时间是1902年。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>下午三点整，参加完会议后返回酒店。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>继亚马逊线下书店开张后,当当网也准备从线上走向线下，这一举动让不少读者感到意外。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>通过这次学习，使大家对党的十九届五中全会精神有了更深层次的理解和认识。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>0</td>\n",
       "      <td>代表们认为，政府只有不断改善民生，社会才能真正发展起来。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>0</td>\n",
       "      <td>近日，一张“重庆大学”学生在宿舍内与他人发生肢体冲突的照片在网上流传。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>0</td>\n",
       "      <td>我觉得这个答复和对这些问题的调查处理，都与我的工作有关。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>1</td>\n",
       "      <td>你叫的外卖送餐箱卫生吗？现实生活中好多叫外卖的老友都是直接从外卖哥的手中接过快餐，很少有人会...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>0</td>\n",
       "      <td>各有关部门要把“扫黄打非”工作作为落实意识形态工作责任制，加强组织领导，形成强大合力的重要举措。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7496 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  为了营造和谐、安全的校园环境，确保广大未成年人的交通安全，天元区公安局交警大队在辖区范围内开...\n",
       "1         0        在翻阅中国话剧100周年纪念活动资料时，他发现《红楼梦》这部戏的演出时间是1902年。\n",
       "2         1                                  下午三点整，参加完会议后返回酒店。\n",
       "3         0           继亚马逊线下书店开张后,当当网也准备从线上走向线下，这一举动让不少读者感到意外。\n",
       "4         1                通过这次学习，使大家对党的十九届五中全会精神有了更深层次的理解和认识。\n",
       "...     ...                                                ...\n",
       "7491      0                       代表们认为，政府只有不断改善民生，社会才能真正发展起来。\n",
       "7492      0                近日，一张“重庆大学”学生在宿舍内与他人发生肢体冲突的照片在网上流传。\n",
       "7493      0                       我觉得这个答复和对这些问题的调查处理，都与我的工作有关。\n",
       "7494      1  你叫的外卖送餐箱卫生吗？现实生活中好多叫外卖的老友都是直接从外卖哥的手中接过快餐，很少有人会...\n",
       "7495      0   各有关部门要把“扫黄打非”工作作为落实意识形态工作责任制，加强组织领导，形成强大合力的重要举措。\n",
       "\n",
       "[7496 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_df = pd.DataFrame(data={'label':probs[0].argmax(1), 'text':texts})\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2982765485467266"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_expanded = pd.concat((train_df, generated_df))\n",
    "(train_df_expanded.label == 0).sum() / len(train_df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25387318496253897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.label == 0).sum() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/data-gen/org/train.csv', sep='\\t', index=False)\n",
    "train_df_expanded.to_csv('../data/data-gen/expanded/train.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
