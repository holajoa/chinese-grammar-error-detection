{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import nlpcda\n",
    "import os\n",
    "\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self, configs:Dict[str, dict]) -> None:\n",
    "        self.entity_swap, self.random_del = None, None\n",
    "\n",
    "        if 'random_entity' in configs.keys():\n",
    "            self.entity_swap_p = configs['random_entity'].pop('prop')\n",
    "            self.entity_swap = nlpcda.Similarword(**(configs['random_entity']))\n",
    "        if 'random_delete_char' in configs.keys():\n",
    "            self.random_del_p = configs['random_delete_char'].pop('prop')\n",
    "            self.random_del = nlpcda.RandomDeleteChar(**(configs['random_delete_char']))\n",
    "    \n",
    "    def aug(self, df_full:pd.DataFrame, permute=True, seed=1117) -> pd.DataFrame:\n",
    "        df = df_full[df_full.label == 1]\n",
    "        df_neg = df_full[df_full.label == 0]\n",
    "\n",
    "        L = len(df)\n",
    "        if self.entity_swap:\n",
    "            augmented_df = self.aug_single(df, L, self.entity_swap_p, self.entity_swap)\n",
    "        if self.random_del:\n",
    "            augmented_df = self.aug_single(augmented_df, L, self.random_del_p, self.random_del)\n",
    "\n",
    "        augmented_df = pd.concat((df_neg, augmented_df))\n",
    "        if permute:\n",
    "            augmented_df = augmented_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        return augmented_df\n",
    "\n",
    "    def aug_single(self, df:pd.DataFrame, L:int, p:float, tool) -> pd.DataFrame:\n",
    "        \"\"\"input L: original df length. Avoid augmentation on newly constructed data. \"\"\"\n",
    "        idx = np.random.choice(range(L), size=int(L*p))\n",
    "        slice_df = df.iloc[idx]\n",
    "        label, text = slice_df[['label', 'text']].values.T\n",
    "        transformed_slice_df = self.get_transformed_df(slice_df, tool)\n",
    "        augmented_df = pd.concat((df, transformed_slice_df))\n",
    "        return augmented_df\n",
    "\n",
    "    def text_seq_transform(self, tool, texts:List[str]) -> List[str]:\n",
    "        out = []\n",
    "        for text in texts:\n",
    "            transformed_text = tool.replace(text)[-1]\n",
    "            out.append(transformed_text)\n",
    "        return np.array(out)\n",
    "    \n",
    "    def get_transformed_df(self, slice_df:pd.DataFrame, tool) -> pd.DataFrame:\n",
    "        label, text = slice_df[['label', 'text']].values.T\n",
    "        transformed_text = self.text_seq_transform(self.entity_swap, text)\n",
    "        transformed_slice_df = slice_df.drop(columns=['text']).copy(deep=True)\n",
    "        transformed_slice_df['text'] = transformed_text\n",
    "        return transformed_slice_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1117)\n",
    "rnd_idx = np.random.choice(range(1, 43001), size=1000)\n",
    "\n",
    "train_df = pd.read_csv('../data/train.csv', sep='\\t')    #.iloc[rnd_idx]\n",
    "test_df = pd.read_csv('../data/test.csv', sep='\\t')\n",
    "\n",
    "train_df.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load :D:\\Apps\\Anaconda3\\envs\\general-torch\\Lib\\site-packages\\nlpcda\\data\\entities.txt done\n"
     ]
    }
   ],
   "source": [
    "entities_file = os.path.join(\"D:\\Apps\\Anaconda3\\envs\\general-torch\\Lib\\site-packages\", \"nlpcda\\data\\entities.txt\")\n",
    "\n",
    "da_configs = {\n",
    "    'random_entity':{\n",
    "        'base_file':entities_file, \n",
    "        'create_num':2, \n",
    "        'change_rate':0.5, \n",
    "        'seed':1024, \n",
    "        'prop':0.3, \n",
    "    }, \n",
    "    'random_delete_char':{\n",
    "        'create_num':2, \n",
    "        'change_rate':0.05, \n",
    "        'seed':1024, \n",
    "        'prop':0.1, \n",
    "    }\n",
    "}\n",
    "\n",
    "da = DataAugmentation(da_configs)\n",
    "train_df_aug = da.aug(train_df)\n",
    "\n",
    "ntf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24324095620901207"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_aug[train_df_aug.label == 0]) / len(train_df_aug[train_df_aug.label == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>通过大力发展社区教育，使我省全民终身学习的教育体系已深入人心。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>再次投入巨资的英超劲旅曼城队能否在2010-2011年度的英超联赛中夺得英超冠军，曼联、切尔...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>广西居民纸质图书的阅读率偏低，手机阅读将成为了广西居民极倾向的阅读方式。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>文字书写时代即将结束，预示着人与字之间最亲密的一种关系已经终结。与此同时，屏幕文化造就了另一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>安徽合力公司2006年叉车销售强劲，销售收入涨幅很有可能将超过40%以上。公司预计2006年...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45258</th>\n",
       "      <td>26096-1</td>\n",
       "      <td>1</td>\n",
       "      <td>他回忆说，小时候在北京，那个时候其实沙尘也很大，戴着口罩骑车去上学。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45259</th>\n",
       "      <td>26096-2</td>\n",
       "      <td>0</td>\n",
       "      <td>到学校之后，口罩上都是厚厚的黄沙子。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45260</th>\n",
       "      <td>26096-3</td>\n",
       "      <td>0</td>\n",
       "      <td>到了冬天，加上煤烟气，情况就更糟了，那个时候没有PM2．5，但是有PM250。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45261</th>\n",
       "      <td>41597-0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005年11月27日，龙煤矿业集团有限责任公司东风煤矿发生一起特大煤尘爆炸事故，死亡171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45262</th>\n",
       "      <td>41597-1</td>\n",
       "      <td>1</td>\n",
       "      <td>这起事故的直接原因是放炮人员使用非专用炸药违章作业处理煤仓堵塞，导致煤尘飞扬达到爆炸界限，放...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label                                               text\n",
       "0            1      1                    通过大力发展社区教育，使我省全民终身学习的教育体系已深入人心。\n",
       "1            2      1  再次投入巨资的英超劲旅曼城队能否在2010-2011年度的英超联赛中夺得英超冠军，曼联、切尔...\n",
       "2            3      1               广西居民纸质图书的阅读率偏低，手机阅读将成为了广西居民极倾向的阅读方式。\n",
       "3            4      1  文字书写时代即将结束，预示着人与字之间最亲密的一种关系已经终结。与此同时，屏幕文化造就了另一...\n",
       "4            5      1  安徽合力公司2006年叉车销售强劲，销售收入涨幅很有可能将超过40%以上。公司预计2006年...\n",
       "...        ...    ...                                                ...\n",
       "45258  26096-1      1                 他回忆说，小时候在北京，那个时候其实沙尘也很大，戴着口罩骑车去上学。\n",
       "45259  26096-2      0                                 到学校之后，口罩上都是厚厚的黄沙子。\n",
       "45260  26096-3      0            到了冬天，加上煤烟气，情况就更糟了，那个时候没有PM2．5，但是有PM250。\n",
       "45261  41597-0      0  2005年11月27日，龙煤矿业集团有限责任公司东风煤矿发生一起特大煤尘爆炸事故，死亡171...\n",
       "45262  41597-1      1  这起事故的直接原因是放炮人员使用非专用炸药违章作业处理煤仓堵塞，导致煤尘飞扬达到爆炸界限，放...\n",
       "\n",
       "[45263 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load :D:\\Apps\\Anaconda3\\envs\\general-torch\\Lib\\site-packages\\nlpcda\\data\\entities.txt done\n"
     ]
    }
   ],
   "source": [
    "entities_file = os.path.join(\"D:\\Apps\\Anaconda3\\envs\\general-torch\\Lib\\site-packages\", \"nlpcda\\data\\entities.txt\")\n",
    "\n",
    "smw = nlpcda.Similarword(base_file=entities_file, create_num=2, change_rate=0.3, seed=1024)\n",
    "dw = nlpcda.RandomDeleteChar(create_num=2, change_rate=0.05, seed=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 0\n",
      "93    航天科学家曾希望这种撞击会激起六英里高的月球尘埃和碎片云，通过对它们扫描可以找到水冰的证据，...\n",
      "94    建设部要求，各地要把风景名胜资源保护工作放在极其重要的位置，采取切实有效的措施，保护风景名胜...\n",
      "95         萧山区青年歌手大赛成为全区百姓最关注的本土声乐大赛，每届都吸引着数百位歌唱爱好者的参与。\n",
      "96         中国古代小说有其自身的特点，重故事，重描写，与西方小说和现代某些中国小说重心理刻画不同。\n",
      "97    《不动产登记暂行条例》按照物权法的有关规定，把登记资料查询人限定在权利人和利害关系人，有关国...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\da.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/da.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel = \u001b[39m\u001b[39m{\u001b[39;00mtrain_df\u001b[39m.\u001b[39miloc[i]\u001b[39m.\u001b[39mlabel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/da.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/da.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(smw\u001b[39m.\u001b[39;49mreplace(text)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/da.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(dw\u001b[39m.\u001b[39mreplace(text)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\nlpcda\\tools\\Similar_word.py:34\u001b[0m, in \u001b[0;36mSimilarword.replace\u001b[1;34m(self, replace_str)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplace\u001b[39m(\u001b[39mself\u001b[39m, replace_str:\u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     replace_str \u001b[39m=\u001b[39m replace_str\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mstrip()\n\u001b[0;32m     35\u001b[0m     seg_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjieba\u001b[39m.\u001b[39mcut(replace_str, cut_all\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m     words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(seg_list)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "i = 93\n",
    "\n",
    "text = train_df.iloc[i:i+5].text\n",
    "print(f'label = {train_df.iloc[i].label}')\n",
    "print(text)\n",
    "print(smw.replace(text)[-1])\n",
    "print(dw.replace(text)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_ds(outputs:List[List[dict]]):\n",
    "    entity_vocab = {}\n",
    "    for output in outputs:\n",
    "        if output:\n",
    "            sentence_vocab = postprocess_sentence(output)\n",
    "            for k, v in sentence_vocab.items():\n",
    "                if k in entity_vocab.keys():\n",
    "                    entity_vocab[k].extend(v)\n",
    "                else:\n",
    "                    entity_vocab[k] = v\n",
    "    return entity_vocab\n",
    "\n",
    "def postprocess_sentence(ner_outputs:List[dict]):\n",
    "    entity_vocab = {}\n",
    "    if ner_outputs == []:\n",
    "        return\n",
    "\n",
    "    current = ''\n",
    "    for out in ner_outputs:\n",
    "        if out['entity'][0] == 'B':\n",
    "            if current:\n",
    "                if category in entity_vocab.keys() and current not in entity_vocab[category]:\n",
    "                    entity_vocab[category].append(current)\n",
    "                else:\n",
    "                    entity_vocab[category] = [current]\n",
    "                current = ''\n",
    "            category = out['entity'][2:]\n",
    "            current += out['word']\n",
    "        if out['entity'][0] == 'I':\n",
    "            if not current:\n",
    "                continue\n",
    "            current += out['word']\n",
    "    if current:\n",
    "        if category in entity_vocab.keys() and current not in entity_vocab[category]:\n",
    "            entity_vocab[category].append(current)\n",
    "        else:\n",
    "            entity_vocab[category] = [current]\n",
    "    return entity_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'postprocess_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\da.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/da.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m postprocess_ds(ner_outputs)[\u001b[39m'\u001b[39m\u001b[39mscene\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'postprocess_ds' is not defined"
     ]
    }
   ],
   "source": [
    "postprocess_ds(ner_outputs)['scene']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
