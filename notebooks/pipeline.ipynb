{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils import ntf\n",
    "from pipeline import PipelineGED\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = 'base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = [\n",
    "#     '../finetuned_models/ner_run_aug_74866/fold0/checkpoint-4065/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold1/checkpoint-8130/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold2/checkpoint-4065/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold3/checkpoint-8130/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold4/checkpoint-4065/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold5/checkpoint-4065/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold6/checkpoint-8130/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold7/checkpoint-8130/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold8/checkpoint-4065/pytorch_model.bin', \n",
    "#     '../finetuned_models/ner_run_aug_74866/fold9/checkpoint-8130/pytorch_model.bin'\n",
    "# ]\n",
    "\n",
    "checkpoints = [f'../finetuned_models/balanced_trial/fold{i}/checkpoint-2865/pytorch_model.bin' for i in range(10)]\n",
    "\n",
    "model_name = 'hfl/chinese-macbert-base' if MODEL_SIZE == 'base' else 'hfl/chinese-macbert-large'\n",
    "max_length = 128 if MODEL_SIZE == 'base' else 64\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "data_configs = {\n",
    "    'model_name':model_name,\n",
    "    'maxlength':max_length,\n",
    "    'train_val_split':-1,\n",
    "    'test':True, \n",
    "    'remove_username':False,\n",
    "    'remove_punctuation':False, \n",
    "    'to_simplified':False, \n",
    "    'emoji_to_text':False, \n",
    "    'split_words':False, \n",
    "    'cut_all':False, \n",
    "}\n",
    "clf = PipelineGED(model_name=model_name, data_configs=data_configs)\n",
    "\n",
    "def apply_ged_pipeline(texts):\n",
    "    probs, seq_probs, err_char = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True, display=True)\n",
    "    return probs, seq_probs, err_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoModelWithClassificationHead:\n\tMissing key(s) in state_dict: \"base_model.embeddings.position_ids\", \"base_model.embeddings.word_embeddings.weight\", \"base_model.embeddings.position_embeddings.weight\", \"base_model.embeddings.token_type_embeddings.weight\", \"base_model.embeddings.LayerNorm.weight\", \"base_model.embeddings.LayerNorm.bias\", \"base_model.encoder.layer.0.attention.self.query.weight\", \"base_model.encoder.layer.0.attention.self.query.bias\", \"base_model.encoder.layer.0.attention.self.key.weight\", \"base_model.encoder.layer.0.attention.self.key.bias\", \"base_model.encoder.layer.0.attention.self.value.weight\", \"base_model.encoder.layer.0.attention.self.value.bias\", \"base_model.encoder.layer.0.attention.output.dense.weight\", \"base_model.encoder.layer.0.attention.output.dense.bias\", \"base_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.0.intermediate.dense.weight\", \"base_model.encoder.layer.0.intermediate.dense.bias\", \"base_model.encoder.layer.0.output.dense.weight\", \"base_model.encoder.layer.0.output.dense.bias\", \"base_model.encoder.layer.0.output.LayerNorm.weight\", \"base_model.encoder.layer.0.output.LayerNorm.bias\", \"base_model.encoder.layer.1.attention.self.query.weight\", \"base_model.encoder.layer.1.attention.self.query.bias\", \"base_model.encoder.layer.1.attention.self.key.weight\", \"base_model.encoder.layer.1.attention.self.key.bias\", \"base_model.encoder.layer.1.attention.self.value.weight\", \"base_model.encoder.layer.1.attention.self.value.bias\", \"base_model.encoder.layer.1.attention.output.dense.weight\", \"base_model.encoder.layer.1.attention.output.dense.bias\", \"base_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.1.intermediate.dense.weight\", \"base_model.encoder.layer.1.intermediate.dense.bias\", \"base_model.encoder.layer.1.output.dense.weight\", \"base_model.encoder.layer.1.output.dense.bias\", \"base_model.encoder.layer.1.output.LayerNorm.weight\", \"base_model.encoder.layer.1.output.LayerNorm.bias\", \"base_model.encoder.layer.2.attention.self.query.weight\", \"base_model.encoder.layer.2.attention.self.query.bias\", \"base_model.encoder.layer.2.attention.self.key.weight\", \"base_model.encoder.layer.2.attention.self.key.bias\", \"base_model.encoder.layer.2.attention.self.value.weight\", \"base_model.encoder.layer.2.attention.self.value.bias\", \"base_model.encoder.layer.2.attention.output.dense.weight\", \"base_model.encoder.layer.2.attention.output.dense.bias\", \"base_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.2.intermediate.dense.weight\", \"base_model.encoder.layer.2.intermediate.dense.bias\", \"base_model.encoder.layer.2.output.dense.weight\", \"base_model.encoder.layer.2.output.dense.bias\", \"base_model.encoder.layer.2.output.LayerNorm.weight\", \"base_model.encoder.layer.2.output.LayerNorm.bias\", \"base_model.encoder.layer.3.attention.self.query.weight\", \"base_model.encoder.layer.3.attention.self.query.bias\", \"base_model.encoder.layer.3.attention.self.key.weight\", \"base_model.encoder.layer.3.attention.self.key.bias\", \"base_model.encoder.layer.3.attention.self.value.weight\", \"base_model.encoder.layer.3.attention.self.value.bias\", \"base_model.encoder.layer.3.attention.output.dense.weight\", \"base_model.encoder.layer.3.attention.output.dense.bias\", \"base_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.3.intermediate.dense.weight\", \"base_model.encoder.layer.3.intermediate.dense.bias\", \"base_model.encoder.layer.3.output.dense.weight\", \"base_model.encoder.layer.3.output.dense.bias\", \"base_model.encoder.layer.3.output.LayerNorm.weight\", \"base_model.encoder.layer.3.output.LayerNorm.bias\", \"base_model.encoder.layer.4.attention.self.query.weight\", \"base_model.encoder.layer.4.attention.self.query.bias\", \"base_model.encoder.layer.4.attention.self.key.weight\", \"base_model.encoder.layer.4.attention.self.key.bias\", \"base_model.encoder.layer.4.attention.self.value.weight\", \"base_model.encoder.layer.4.attention.self.value.bias\", \"base_model.encoder.layer.4.attention.output.dense.weight\", \"base_model.encoder.layer.4.attention.output.dense.bias\", \"base_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.4.intermediate.dense.weight\", \"base_model.encoder.layer.4.intermediate.dense.bias\", \"base_model.encoder.layer.4.output.dense.weight\", \"base_model.encoder.layer.4.output.dense.bias\", \"base_model.encoder.layer.4.output.LayerNorm.weight\", \"base_model.encoder.layer.4.output.LayerNorm.bias\", \"base_model.encoder.layer.5.attention.self.query.weight\", \"base_model.encoder.layer.5.attention.self.query.bias\", \"base_model.encoder.layer.5.attention.self.key.weight\", \"base_model.encoder.layer.5.attention.self.key.bias\", \"base_model.encoder.layer.5.attention.self.value.weight\", \"base_model.encoder.layer.5.attention.self.value.bias\", \"base_model.encoder.layer.5.attention.output.dense.weight\", \"base_model.encoder.layer.5.attention.output.dense.bias\", \"base_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.5.intermediate.dense.weight\", \"base_model.encoder.layer.5.intermediate.dense.bias\", \"base_model.encoder.layer.5.output.dense.weight\", \"base_model.encoder.layer.5.output.dense.bias\", \"base_model.encoder.layer.5.output.LayerNorm.weight\", \"base_model.encoder.layer.5.output.LayerNorm.bias\", \"base_model.encoder.layer.6.attention.self.query.weight\", \"base_model.encoder.layer.6.attention.self.query.bias\", \"base_model.encoder.layer.6.attention.self.key.weight\", \"base_model.encoder.layer.6.attention.self.key.bias\", \"base_model.encoder.layer.6.attention.self.value.weight\", \"base_model.encoder.layer.6.attention.self.value.bias\", \"base_model.encoder.layer.6.attention.output.dense.weight\", \"base_model.encoder.layer.6.attention.output.dense.bias\", \"base_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.6.intermediate.dense.weight\", \"base_model.encoder.layer.6.intermediate.dense.bias\", \"base_model.encoder.layer.6.output.dense.weight\", \"base_model.encoder.layer.6.output.dense.bias\", \"base_model.encoder.layer.6.output.LayerNorm.weight\", \"base_model.encoder.layer.6.output.LayerNorm.bias\", \"base_model.encoder.layer.7.attention.self.query.weight\", \"base_model.encoder.layer.7.attention.self.query.bias\", \"base_model.encoder.layer.7.attention.self.key.weight\", \"base_model.encoder.layer.7.attention.self.key.bias\", \"base_model.encoder.layer.7.attention.self.value.weight\", \"base_model.encoder.layer.7.attention.self.value.bias\", \"base_model.encoder.layer.7.attention.output.dense.weight\", \"base_model.encoder.layer.7.attention.output.dense.bias\", \"base_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.7.intermediate.dense.weight\", \"base_model.encoder.layer.7.intermediate.dense.bias\", \"base_model.encoder.layer.7.output.dense.weight\", \"base_model.encoder.layer.7.output.dense.bias\", \"base_model.encoder.layer.7.output.LayerNorm.weight\", \"base_model.encoder.layer.7.output.LayerNorm.bias\", \"base_model.encoder.layer.8.attention.self.query.weight\", \"base_model.encoder.layer.8.attention.self.query.bias\", \"base_model.encoder.layer.8.attention.self.key.weight\", \"base_model.encoder.layer.8.attention.self.key.bias\", \"base_model.encoder.layer.8.attention.self.value.weight\", \"base_model.encoder.layer.8.attention.self.value.bias\", \"base_model.encoder.layer.8.attention.output.dense.weight\", \"base_model.encoder.layer.8.attention.output.dense.bias\", \"base_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.8.intermediate.dense.weight\", \"base_model.encoder.layer.8.intermediate.dense.bias\", \"base_model.encoder.layer.8.output.dense.weight\", \"base_model.encoder.layer.8.output.dense.bias\", \"base_model.encoder.layer.8.output.LayerNorm.weight\", \"base_model.encoder.layer.8.output.LayerNorm.bias\", \"base_model.encoder.layer.9.attention.self.query.weight\", \"base_model.encoder.layer.9.attention.self.query.bias\", \"base_model.encoder.layer.9.attention.self.key.weight\", \"base_model.encoder.layer.9.attention.self.key.bias\", \"base_model.encoder.layer.9.attention.self.value.weight\", \"base_model.encoder.layer.9.attention.self.value.bias\", \"base_model.encoder.layer.9.attention.output.dense.weight\", \"base_model.encoder.layer.9.attention.output.dense.bias\", \"base_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.9.intermediate.dense.weight\", \"base_model.encoder.layer.9.intermediate.dense.bias\", \"base_model.encoder.layer.9.output.dense.weight\", \"base_model.encoder.layer.9.output.dense.bias\", \"base_model.encoder.layer.9.output.LayerNorm.weight\", \"base_model.encoder.layer.9.output.LayerNorm.bias\", \"base_model.encoder.layer.10.attention.self.query.weight\", \"base_model.encoder.layer.10.attention.self.query.bias\", \"base_model.encoder.layer.10.attention.self.key.weight\", \"base_model.encoder.layer.10.attention.self.key.bias\", \"base_model.encoder.layer.10.attention.self.value.weight\", \"base_model.encoder.layer.10.attention.self.value.bias\", \"base_model.encoder.layer.10.attention.output.dense.weight\", \"base_model.encoder.layer.10.attention.output.dense.bias\", \"base_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.10.intermediate.dense.weight\", \"base_model.encoder.layer.10.intermediate.dense.bias\", \"base_model.encoder.layer.10.output.dense.weight\", \"base_model.encoder.layer.10.output.dense.bias\", \"base_model.encoder.layer.10.output.LayerNorm.weight\", \"base_model.encoder.layer.10.output.LayerNorm.bias\", \"base_model.encoder.layer.11.attention.self.query.weight\", \"base_model.encoder.layer.11.attention.self.query.bias\", \"base_model.encoder.layer.11.attention.self.key.weight\", \"base_model.encoder.layer.11.attention.self.key.bias\", \"base_model.encoder.layer.11.attention.self.value.weight\", \"base_model.encoder.layer.11.attention.self.value.bias\", \"base_model.encoder.layer.11.attention.output.dense.weight\", \"base_model.encoder.layer.11.attention.output.dense.bias\", \"base_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.11.intermediate.dense.weight\", \"base_model.encoder.layer.11.intermediate.dense.bias\", \"base_model.encoder.layer.11.output.dense.weight\", \"base_model.encoder.layer.11.output.dense.bias\", \"base_model.encoder.layer.11.output.LayerNorm.weight\", \"base_model.encoder.layer.11.output.LayerNorm.bias\", \"base_model.pooler.dense.weight\", \"base_model.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"base_model.bert.embeddings.position_ids\", \"base_model.bert.embeddings.word_embeddings.weight\", \"base_model.bert.embeddings.position_embeddings.weight\", \"base_model.bert.embeddings.token_type_embeddings.weight\", \"base_model.bert.embeddings.LayerNorm.weight\", \"base_model.bert.embeddings.LayerNorm.bias\", \"base_model.bert.encoder.layer.0.attention.self.query.weight\", \"base_model.bert.encoder.layer.0.attention.self.query.bias\", \"base_model.bert.encoder.layer.0.attention.self.key.weight\", \"base_model.bert.encoder.layer.0.attention.self.key.bias\", \"base_model.bert.encoder.layer.0.attention.self.value.weight\", \"base_model.bert.encoder.layer.0.attention.self.value.bias\", \"base_model.bert.encoder.layer.0.attention.output.dense.weight\", \"base_model.bert.encoder.layer.0.attention.output.dense.bias\", \"base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.0.intermediate.dense.weight\", \"base_model.bert.encoder.layer.0.intermediate.dense.bias\", \"base_model.bert.encoder.layer.0.output.dense.weight\", \"base_model.bert.encoder.layer.0.output.dense.bias\", \"base_model.bert.encoder.layer.0.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.0.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.1.attention.self.query.weight\", \"base_model.bert.encoder.layer.1.attention.self.query.bias\", \"base_model.bert.encoder.layer.1.attention.self.key.weight\", \"base_model.bert.encoder.layer.1.attention.self.key.bias\", \"base_model.bert.encoder.layer.1.attention.self.value.weight\", \"base_model.bert.encoder.layer.1.attention.self.value.bias\", \"base_model.bert.encoder.layer.1.attention.output.dense.weight\", \"base_model.bert.encoder.layer.1.attention.output.dense.bias\", \"base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.1.intermediate.dense.weight\", \"base_model.bert.encoder.layer.1.intermediate.dense.bias\", \"base_model.bert.encoder.layer.1.output.dense.weight\", \"base_model.bert.encoder.layer.1.output.dense.bias\", \"base_model.bert.encoder.layer.1.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.1.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.2.attention.self.query.weight\", \"base_model.bert.encoder.layer.2.attention.self.query.bias\", \"base_model.bert.encoder.layer.2.attention.self.key.weight\", \"base_model.bert.encoder.layer.2.attention.self.key.bias\", \"base_model.bert.encoder.layer.2.attention.self.value.weight\", \"base_model.bert.encoder.layer.2.attention.self.value.bias\", \"base_model.bert.encoder.layer.2.attention.output.dense.weight\", \"base_model.bert.encoder.layer.2.attention.output.dense.bias\", \"base_model.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.2.intermediate.dense.weight\", \"base_model.bert.encoder.layer.2.intermediate.dense.bias\", \"base_model.bert.encoder.layer.2.output.dense.weight\", \"base_model.bert.encoder.layer.2.output.dense.bias\", \"base_model.bert.encoder.layer.2.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.2.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.3.attention.self.query.weight\", \"base_model.bert.encoder.layer.3.attention.self.query.bias\", \"base_model.bert.encoder.layer.3.attention.self.key.weight\", \"base_model.bert.encoder.layer.3.attention.self.key.bias\", \"base_model.bert.encoder.layer.3.attention.self.value.weight\", \"base_model.bert.encoder.layer.3.attention.self.value.bias\", \"base_model.bert.encoder.layer.3.attention.output.dense.weight\", \"base_model.bert.encoder.layer.3.attention.output.dense.bias\", \"base_model.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.3.intermediate.dense.weight\", \"base_model.bert.encoder.layer.3.intermediate.dense.bias\", \"base_model.bert.encoder.layer.3.output.dense.weight\", \"base_model.bert.encoder.layer.3.output.dense.bias\", \"base_model.bert.encoder.layer.3.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.3.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.4.attention.self.query.weight\", \"base_model.bert.encoder.layer.4.attention.self.query.bias\", \"base_model.bert.encoder.layer.4.attention.self.key.weight\", \"base_model.bert.encoder.layer.4.attention.self.key.bias\", \"base_model.bert.encoder.layer.4.attention.self.value.weight\", \"base_model.bert.encoder.layer.4.attention.self.value.bias\", \"base_model.bert.encoder.layer.4.attention.output.dense.weight\", \"base_model.bert.encoder.layer.4.attention.output.dense.bias\", \"base_model.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.4.intermediate.dense.weight\", \"base_model.bert.encoder.layer.4.intermediate.dense.bias\", \"base_model.bert.encoder.layer.4.output.dense.weight\", \"base_model.bert.encoder.layer.4.output.dense.bias\", \"base_model.bert.encoder.layer.4.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.4.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.5.attention.self.query.weight\", \"base_model.bert.encoder.layer.5.attention.self.query.bias\", \"base_model.bert.encoder.layer.5.attention.self.key.weight\", \"base_model.bert.encoder.layer.5.attention.self.key.bias\", \"base_model.bert.encoder.layer.5.attention.self.value.weight\", \"base_model.bert.encoder.layer.5.attention.self.value.bias\", \"base_model.bert.encoder.layer.5.attention.output.dense.weight\", \"base_model.bert.encoder.layer.5.attention.output.dense.bias\", \"base_model.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.5.intermediate.dense.weight\", \"base_model.bert.encoder.layer.5.intermediate.dense.bias\", \"base_model.bert.encoder.layer.5.output.dense.weight\", \"base_model.bert.encoder.layer.5.output.dense.bias\", \"base_model.bert.encoder.layer.5.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.5.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.6.attention.self.query.weight\", \"base_model.bert.encoder.layer.6.attention.self.query.bias\", \"base_model.bert.encoder.layer.6.attention.self.key.weight\", \"base_model.bert.encoder.layer.6.attention.self.key.bias\", \"base_model.bert.encoder.layer.6.attention.self.value.weight\", \"base_model.bert.encoder.layer.6.attention.self.value.bias\", \"base_model.bert.encoder.layer.6.attention.output.dense.weight\", \"base_model.bert.encoder.layer.6.attention.output.dense.bias\", \"base_model.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.6.intermediate.dense.weight\", \"base_model.bert.encoder.layer.6.intermediate.dense.bias\", \"base_model.bert.encoder.layer.6.output.dense.weight\", \"base_model.bert.encoder.layer.6.output.dense.bias\", \"base_model.bert.encoder.layer.6.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.6.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.7.attention.self.query.weight\", \"base_model.bert.encoder.layer.7.attention.self.query.bias\", \"base_model.bert.encoder.layer.7.attention.self.key.weight\", \"base_model.bert.encoder.layer.7.attention.self.key.bias\", \"base_model.bert.encoder.layer.7.attention.self.value.weight\", \"base_model.bert.encoder.layer.7.attention.self.value.bias\", \"base_model.bert.encoder.layer.7.attention.output.dense.weight\", \"base_model.bert.encoder.layer.7.attention.output.dense.bias\", \"base_model.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.7.intermediate.dense.weight\", \"base_model.bert.encoder.layer.7.intermediate.dense.bias\", \"base_model.bert.encoder.layer.7.output.dense.weight\", \"base_model.bert.encoder.layer.7.output.dense.bias\", \"base_model.bert.encoder.layer.7.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.7.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.8.attention.self.query.weight\", \"base_model.bert.encoder.layer.8.attention.self.query.bias\", \"base_model.bert.encoder.layer.8.attention.self.key.weight\", \"base_model.bert.encoder.layer.8.attention.self.key.bias\", \"base_model.bert.encoder.layer.8.attention.self.value.weight\", \"base_model.bert.encoder.layer.8.attention.self.value.bias\", \"base_model.bert.encoder.layer.8.attention.output.dense.weight\", \"base_model.bert.encoder.layer.8.attention.output.dense.bias\", \"base_model.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.8.intermediate.dense.weight\", \"base_model.bert.encoder.layer.8.intermediate.dense.bias\", \"base_model.bert.encoder.layer.8.output.dense.weight\", \"base_model.bert.encoder.layer.8.output.dense.bias\", \"base_model.bert.encoder.layer.8.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.8.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.9.attention.self.query.weight\", \"base_model.bert.encoder.layer.9.attention.self.query.bias\", \"base_model.bert.encoder.layer.9.attention.self.key.weight\", \"base_model.bert.encoder.layer.9.attention.self.key.bias\", \"base_model.bert.encoder.layer.9.attention.self.value.weight\", \"base_model.bert.encoder.layer.9.attention.self.value.bias\", \"base_model.bert.encoder.layer.9.attention.output.dense.weight\", \"base_model.bert.encoder.layer.9.attention.output.dense.bias\", \"base_model.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.9.intermediate.dense.weight\", \"base_model.bert.encoder.layer.9.intermediate.dense.bias\", \"base_model.bert.encoder.layer.9.output.dense.weight\", \"base_model.bert.encoder.layer.9.output.dense.bias\", \"base_model.bert.encoder.layer.9.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.9.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.10.attention.self.query.weight\", \"base_model.bert.encoder.layer.10.attention.self.query.bias\", \"base_model.bert.encoder.layer.10.attention.self.key.weight\", \"base_model.bert.encoder.layer.10.attention.self.key.bias\", \"base_model.bert.encoder.layer.10.attention.self.value.weight\", \"base_model.bert.encoder.layer.10.attention.self.value.bias\", \"base_model.bert.encoder.layer.10.attention.output.dense.weight\", \"base_model.bert.encoder.layer.10.attention.output.dense.bias\", \"base_model.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.10.intermediate.dense.weight\", \"base_model.bert.encoder.layer.10.intermediate.dense.bias\", \"base_model.bert.encoder.layer.10.output.dense.weight\", \"base_model.bert.encoder.layer.10.output.dense.bias\", \"base_model.bert.encoder.layer.10.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.10.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.11.attention.self.query.weight\", \"base_model.bert.encoder.layer.11.attention.self.query.bias\", \"base_model.bert.encoder.layer.11.attention.self.key.weight\", \"base_model.bert.encoder.layer.11.attention.self.key.bias\", \"base_model.bert.encoder.layer.11.attention.self.value.weight\", \"base_model.bert.encoder.layer.11.attention.self.value.bias\", \"base_model.bert.encoder.layer.11.attention.output.dense.weight\", \"base_model.bert.encoder.layer.11.attention.output.dense.bias\", \"base_model.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.11.intermediate.dense.weight\", \"base_model.bert.encoder.layer.11.intermediate.dense.bias\", \"base_model.bert.encoder.layer.11.output.dense.weight\", \"base_model.bert.encoder.layer.11.output.dense.bias\", \"base_model.bert.encoder.layer.11.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.11.output.LayerNorm.bias\", \"base_model.cls.predictions.bias\", \"base_model.cls.predictions.transform.dense.weight\", \"base_model.cls.predictions.transform.dense.bias\", \"base_model.cls.predictions.transform.LayerNorm.weight\", \"base_model.cls.predictions.transform.LayerNorm.bias\", \"base_model.cls.predictions.decoder.weight\", \"base_model.cls.predictions.decoder.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\pipeline.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## 搭配\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m texts \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m女娲有一双聪明能干的手。\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m女娲有一双能干的手。\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m女娲有一双聪明的手。\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m probs, seq_probs, err_char \u001b[39m=\u001b[39m apply_ged_pipeline(texts)\n",
      "\u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\pipeline.ipynb Cell 4\u001b[0m in \u001b[0;36mapply_ged_pipeline\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_ged_pipeline\u001b[39m(texts):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     probs, seq_probs, err_char \u001b[39m=\u001b[39m clf(texts\u001b[39m=\u001b[39;49mtexts, checkpoints\u001b[39m=\u001b[39;49mcheckpoints, device\u001b[39m=\u001b[39;49mdevice, output_probabilities\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, display\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Develop/chinese-grammar-error-detection/notebooks/pipeline.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m probs, seq_probs, err_char\n",
      "File \u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\pipeline.py:216\u001b[0m, in \u001b[0;36mPipelineGED.__call__\u001b[1;34m(self, texts, checkpoints, device, raw_outputs, output_probabilities, display, majority_vote)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m majority_vote:\n\u001b[0;32m    215\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(test, checkpoints, device, raw_outputs, output_probabilities, majority_vote\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 216\u001b[0m probs, seq_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeedforward(test, checkpoints, device, raw_outputs, output_probabilities)\n\u001b[0;32m    217\u001b[0m err_char_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay_error_chars(seq_probs, test\u001b[39m.\u001b[39mtexts\u001b[39m.\u001b[39mvalues, display\u001b[39m=\u001b[39mdisplay)\n\u001b[0;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m probs, seq_probs, err_char_lst\n",
      "File \u001b[1;32md:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\pipeline.py:146\u001b[0m, in \u001b[0;36mPipelineGED.feedforward\u001b[1;34m(self, ds, checkpoints, device, raw_outputs, output_probabilities, majority_vote)\u001b[0m\n\u001b[0;32m    143\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(cp, map_location\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    144\u001b[0m \u001b[39m# for key in list(state_dict.keys()):\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m#     state_dict[key.replace('bert', 'base_model')] = state_dict.pop(key)\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(state_dict)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m device\u001b[39m.\u001b[39mtype:\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32md:\\Apps\\Anaconda3\\envs\\general-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoModelWithClassificationHead:\n\tMissing key(s) in state_dict: \"base_model.embeddings.position_ids\", \"base_model.embeddings.word_embeddings.weight\", \"base_model.embeddings.position_embeddings.weight\", \"base_model.embeddings.token_type_embeddings.weight\", \"base_model.embeddings.LayerNorm.weight\", \"base_model.embeddings.LayerNorm.bias\", \"base_model.encoder.layer.0.attention.self.query.weight\", \"base_model.encoder.layer.0.attention.self.query.bias\", \"base_model.encoder.layer.0.attention.self.key.weight\", \"base_model.encoder.layer.0.attention.self.key.bias\", \"base_model.encoder.layer.0.attention.self.value.weight\", \"base_model.encoder.layer.0.attention.self.value.bias\", \"base_model.encoder.layer.0.attention.output.dense.weight\", \"base_model.encoder.layer.0.attention.output.dense.bias\", \"base_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.0.intermediate.dense.weight\", \"base_model.encoder.layer.0.intermediate.dense.bias\", \"base_model.encoder.layer.0.output.dense.weight\", \"base_model.encoder.layer.0.output.dense.bias\", \"base_model.encoder.layer.0.output.LayerNorm.weight\", \"base_model.encoder.layer.0.output.LayerNorm.bias\", \"base_model.encoder.layer.1.attention.self.query.weight\", \"base_model.encoder.layer.1.attention.self.query.bias\", \"base_model.encoder.layer.1.attention.self.key.weight\", \"base_model.encoder.layer.1.attention.self.key.bias\", \"base_model.encoder.layer.1.attention.self.value.weight\", \"base_model.encoder.layer.1.attention.self.value.bias\", \"base_model.encoder.layer.1.attention.output.dense.weight\", \"base_model.encoder.layer.1.attention.output.dense.bias\", \"base_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.1.intermediate.dense.weight\", \"base_model.encoder.layer.1.intermediate.dense.bias\", \"base_model.encoder.layer.1.output.dense.weight\", \"base_model.encoder.layer.1.output.dense.bias\", \"base_model.encoder.layer.1.output.LayerNorm.weight\", \"base_model.encoder.layer.1.output.LayerNorm.bias\", \"base_model.encoder.layer.2.attention.self.query.weight\", \"base_model.encoder.layer.2.attention.self.query.bias\", \"base_model.encoder.layer.2.attention.self.key.weight\", \"base_model.encoder.layer.2.attention.self.key.bias\", \"base_model.encoder.layer.2.attention.self.value.weight\", \"base_model.encoder.layer.2.attention.self.value.bias\", \"base_model.encoder.layer.2.attention.output.dense.weight\", \"base_model.encoder.layer.2.attention.output.dense.bias\", \"base_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.2.intermediate.dense.weight\", \"base_model.encoder.layer.2.intermediate.dense.bias\", \"base_model.encoder.layer.2.output.dense.weight\", \"base_model.encoder.layer.2.output.dense.bias\", \"base_model.encoder.layer.2.output.LayerNorm.weight\", \"base_model.encoder.layer.2.output.LayerNorm.bias\", \"base_model.encoder.layer.3.attention.self.query.weight\", \"base_model.encoder.layer.3.attention.self.query.bias\", \"base_model.encoder.layer.3.attention.self.key.weight\", \"base_model.encoder.layer.3.attention.self.key.bias\", \"base_model.encoder.layer.3.attention.self.value.weight\", \"base_model.encoder.layer.3.attention.self.value.bias\", \"base_model.encoder.layer.3.attention.output.dense.weight\", \"base_model.encoder.layer.3.attention.output.dense.bias\", \"base_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.3.intermediate.dense.weight\", \"base_model.encoder.layer.3.intermediate.dense.bias\", \"base_model.encoder.layer.3.output.dense.weight\", \"base_model.encoder.layer.3.output.dense.bias\", \"base_model.encoder.layer.3.output.LayerNorm.weight\", \"base_model.encoder.layer.3.output.LayerNorm.bias\", \"base_model.encoder.layer.4.attention.self.query.weight\", \"base_model.encoder.layer.4.attention.self.query.bias\", \"base_model.encoder.layer.4.attention.self.key.weight\", \"base_model.encoder.layer.4.attention.self.key.bias\", \"base_model.encoder.layer.4.attention.self.value.weight\", \"base_model.encoder.layer.4.attention.self.value.bias\", \"base_model.encoder.layer.4.attention.output.dense.weight\", \"base_model.encoder.layer.4.attention.output.dense.bias\", \"base_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.4.intermediate.dense.weight\", \"base_model.encoder.layer.4.intermediate.dense.bias\", \"base_model.encoder.layer.4.output.dense.weight\", \"base_model.encoder.layer.4.output.dense.bias\", \"base_model.encoder.layer.4.output.LayerNorm.weight\", \"base_model.encoder.layer.4.output.LayerNorm.bias\", \"base_model.encoder.layer.5.attention.self.query.weight\", \"base_model.encoder.layer.5.attention.self.query.bias\", \"base_model.encoder.layer.5.attention.self.key.weight\", \"base_model.encoder.layer.5.attention.self.key.bias\", \"base_model.encoder.layer.5.attention.self.value.weight\", \"base_model.encoder.layer.5.attention.self.value.bias\", \"base_model.encoder.layer.5.attention.output.dense.weight\", \"base_model.encoder.layer.5.attention.output.dense.bias\", \"base_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.5.intermediate.dense.weight\", \"base_model.encoder.layer.5.intermediate.dense.bias\", \"base_model.encoder.layer.5.output.dense.weight\", \"base_model.encoder.layer.5.output.dense.bias\", \"base_model.encoder.layer.5.output.LayerNorm.weight\", \"base_model.encoder.layer.5.output.LayerNorm.bias\", \"base_model.encoder.layer.6.attention.self.query.weight\", \"base_model.encoder.layer.6.attention.self.query.bias\", \"base_model.encoder.layer.6.attention.self.key.weight\", \"base_model.encoder.layer.6.attention.self.key.bias\", \"base_model.encoder.layer.6.attention.self.value.weight\", \"base_model.encoder.layer.6.attention.self.value.bias\", \"base_model.encoder.layer.6.attention.output.dense.weight\", \"base_model.encoder.layer.6.attention.output.dense.bias\", \"base_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.6.intermediate.dense.weight\", \"base_model.encoder.layer.6.intermediate.dense.bias\", \"base_model.encoder.layer.6.output.dense.weight\", \"base_model.encoder.layer.6.output.dense.bias\", \"base_model.encoder.layer.6.output.LayerNorm.weight\", \"base_model.encoder.layer.6.output.LayerNorm.bias\", \"base_model.encoder.layer.7.attention.self.query.weight\", \"base_model.encoder.layer.7.attention.self.query.bias\", \"base_model.encoder.layer.7.attention.self.key.weight\", \"base_model.encoder.layer.7.attention.self.key.bias\", \"base_model.encoder.layer.7.attention.self.value.weight\", \"base_model.encoder.layer.7.attention.self.value.bias\", \"base_model.encoder.layer.7.attention.output.dense.weight\", \"base_model.encoder.layer.7.attention.output.dense.bias\", \"base_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.7.intermediate.dense.weight\", \"base_model.encoder.layer.7.intermediate.dense.bias\", \"base_model.encoder.layer.7.output.dense.weight\", \"base_model.encoder.layer.7.output.dense.bias\", \"base_model.encoder.layer.7.output.LayerNorm.weight\", \"base_model.encoder.layer.7.output.LayerNorm.bias\", \"base_model.encoder.layer.8.attention.self.query.weight\", \"base_model.encoder.layer.8.attention.self.query.bias\", \"base_model.encoder.layer.8.attention.self.key.weight\", \"base_model.encoder.layer.8.attention.self.key.bias\", \"base_model.encoder.layer.8.attention.self.value.weight\", \"base_model.encoder.layer.8.attention.self.value.bias\", \"base_model.encoder.layer.8.attention.output.dense.weight\", \"base_model.encoder.layer.8.attention.output.dense.bias\", \"base_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.8.intermediate.dense.weight\", \"base_model.encoder.layer.8.intermediate.dense.bias\", \"base_model.encoder.layer.8.output.dense.weight\", \"base_model.encoder.layer.8.output.dense.bias\", \"base_model.encoder.layer.8.output.LayerNorm.weight\", \"base_model.encoder.layer.8.output.LayerNorm.bias\", \"base_model.encoder.layer.9.attention.self.query.weight\", \"base_model.encoder.layer.9.attention.self.query.bias\", \"base_model.encoder.layer.9.attention.self.key.weight\", \"base_model.encoder.layer.9.attention.self.key.bias\", \"base_model.encoder.layer.9.attention.self.value.weight\", \"base_model.encoder.layer.9.attention.self.value.bias\", \"base_model.encoder.layer.9.attention.output.dense.weight\", \"base_model.encoder.layer.9.attention.output.dense.bias\", \"base_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.9.intermediate.dense.weight\", \"base_model.encoder.layer.9.intermediate.dense.bias\", \"base_model.encoder.layer.9.output.dense.weight\", \"base_model.encoder.layer.9.output.dense.bias\", \"base_model.encoder.layer.9.output.LayerNorm.weight\", \"base_model.encoder.layer.9.output.LayerNorm.bias\", \"base_model.encoder.layer.10.attention.self.query.weight\", \"base_model.encoder.layer.10.attention.self.query.bias\", \"base_model.encoder.layer.10.attention.self.key.weight\", \"base_model.encoder.layer.10.attention.self.key.bias\", \"base_model.encoder.layer.10.attention.self.value.weight\", \"base_model.encoder.layer.10.attention.self.value.bias\", \"base_model.encoder.layer.10.attention.output.dense.weight\", \"base_model.encoder.layer.10.attention.output.dense.bias\", \"base_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.10.intermediate.dense.weight\", \"base_model.encoder.layer.10.intermediate.dense.bias\", \"base_model.encoder.layer.10.output.dense.weight\", \"base_model.encoder.layer.10.output.dense.bias\", \"base_model.encoder.layer.10.output.LayerNorm.weight\", \"base_model.encoder.layer.10.output.LayerNorm.bias\", \"base_model.encoder.layer.11.attention.self.query.weight\", \"base_model.encoder.layer.11.attention.self.query.bias\", \"base_model.encoder.layer.11.attention.self.key.weight\", \"base_model.encoder.layer.11.attention.self.key.bias\", \"base_model.encoder.layer.11.attention.self.value.weight\", \"base_model.encoder.layer.11.attention.self.value.bias\", \"base_model.encoder.layer.11.attention.output.dense.weight\", \"base_model.encoder.layer.11.attention.output.dense.bias\", \"base_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"base_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"base_model.encoder.layer.11.intermediate.dense.weight\", \"base_model.encoder.layer.11.intermediate.dense.bias\", \"base_model.encoder.layer.11.output.dense.weight\", \"base_model.encoder.layer.11.output.dense.bias\", \"base_model.encoder.layer.11.output.LayerNorm.weight\", \"base_model.encoder.layer.11.output.LayerNorm.bias\", \"base_model.pooler.dense.weight\", \"base_model.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"base_model.bert.embeddings.position_ids\", \"base_model.bert.embeddings.word_embeddings.weight\", \"base_model.bert.embeddings.position_embeddings.weight\", \"base_model.bert.embeddings.token_type_embeddings.weight\", \"base_model.bert.embeddings.LayerNorm.weight\", \"base_model.bert.embeddings.LayerNorm.bias\", \"base_model.bert.encoder.layer.0.attention.self.query.weight\", \"base_model.bert.encoder.layer.0.attention.self.query.bias\", \"base_model.bert.encoder.layer.0.attention.self.key.weight\", \"base_model.bert.encoder.layer.0.attention.self.key.bias\", \"base_model.bert.encoder.layer.0.attention.self.value.weight\", \"base_model.bert.encoder.layer.0.attention.self.value.bias\", \"base_model.bert.encoder.layer.0.attention.output.dense.weight\", \"base_model.bert.encoder.layer.0.attention.output.dense.bias\", \"base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.0.intermediate.dense.weight\", \"base_model.bert.encoder.layer.0.intermediate.dense.bias\", \"base_model.bert.encoder.layer.0.output.dense.weight\", \"base_model.bert.encoder.layer.0.output.dense.bias\", \"base_model.bert.encoder.layer.0.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.0.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.1.attention.self.query.weight\", \"base_model.bert.encoder.layer.1.attention.self.query.bias\", \"base_model.bert.encoder.layer.1.attention.self.key.weight\", \"base_model.bert.encoder.layer.1.attention.self.key.bias\", \"base_model.bert.encoder.layer.1.attention.self.value.weight\", \"base_model.bert.encoder.layer.1.attention.self.value.bias\", \"base_model.bert.encoder.layer.1.attention.output.dense.weight\", \"base_model.bert.encoder.layer.1.attention.output.dense.bias\", \"base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.1.intermediate.dense.weight\", \"base_model.bert.encoder.layer.1.intermediate.dense.bias\", \"base_model.bert.encoder.layer.1.output.dense.weight\", \"base_model.bert.encoder.layer.1.output.dense.bias\", \"base_model.bert.encoder.layer.1.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.1.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.2.attention.self.query.weight\", \"base_model.bert.encoder.layer.2.attention.self.query.bias\", \"base_model.bert.encoder.layer.2.attention.self.key.weight\", \"base_model.bert.encoder.layer.2.attention.self.key.bias\", \"base_model.bert.encoder.layer.2.attention.self.value.weight\", \"base_model.bert.encoder.layer.2.attention.self.value.bias\", \"base_model.bert.encoder.layer.2.attention.output.dense.weight\", \"base_model.bert.encoder.layer.2.attention.output.dense.bias\", \"base_model.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.2.intermediate.dense.weight\", \"base_model.bert.encoder.layer.2.intermediate.dense.bias\", \"base_model.bert.encoder.layer.2.output.dense.weight\", \"base_model.bert.encoder.layer.2.output.dense.bias\", \"base_model.bert.encoder.layer.2.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.2.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.3.attention.self.query.weight\", \"base_model.bert.encoder.layer.3.attention.self.query.bias\", \"base_model.bert.encoder.layer.3.attention.self.key.weight\", \"base_model.bert.encoder.layer.3.attention.self.key.bias\", \"base_model.bert.encoder.layer.3.attention.self.value.weight\", \"base_model.bert.encoder.layer.3.attention.self.value.bias\", \"base_model.bert.encoder.layer.3.attention.output.dense.weight\", \"base_model.bert.encoder.layer.3.attention.output.dense.bias\", \"base_model.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.3.intermediate.dense.weight\", \"base_model.bert.encoder.layer.3.intermediate.dense.bias\", \"base_model.bert.encoder.layer.3.output.dense.weight\", \"base_model.bert.encoder.layer.3.output.dense.bias\", \"base_model.bert.encoder.layer.3.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.3.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.4.attention.self.query.weight\", \"base_model.bert.encoder.layer.4.attention.self.query.bias\", \"base_model.bert.encoder.layer.4.attention.self.key.weight\", \"base_model.bert.encoder.layer.4.attention.self.key.bias\", \"base_model.bert.encoder.layer.4.attention.self.value.weight\", \"base_model.bert.encoder.layer.4.attention.self.value.bias\", \"base_model.bert.encoder.layer.4.attention.output.dense.weight\", \"base_model.bert.encoder.layer.4.attention.output.dense.bias\", \"base_model.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.4.intermediate.dense.weight\", \"base_model.bert.encoder.layer.4.intermediate.dense.bias\", \"base_model.bert.encoder.layer.4.output.dense.weight\", \"base_model.bert.encoder.layer.4.output.dense.bias\", \"base_model.bert.encoder.layer.4.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.4.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.5.attention.self.query.weight\", \"base_model.bert.encoder.layer.5.attention.self.query.bias\", \"base_model.bert.encoder.layer.5.attention.self.key.weight\", \"base_model.bert.encoder.layer.5.attention.self.key.bias\", \"base_model.bert.encoder.layer.5.attention.self.value.weight\", \"base_model.bert.encoder.layer.5.attention.self.value.bias\", \"base_model.bert.encoder.layer.5.attention.output.dense.weight\", \"base_model.bert.encoder.layer.5.attention.output.dense.bias\", \"base_model.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.5.intermediate.dense.weight\", \"base_model.bert.encoder.layer.5.intermediate.dense.bias\", \"base_model.bert.encoder.layer.5.output.dense.weight\", \"base_model.bert.encoder.layer.5.output.dense.bias\", \"base_model.bert.encoder.layer.5.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.5.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.6.attention.self.query.weight\", \"base_model.bert.encoder.layer.6.attention.self.query.bias\", \"base_model.bert.encoder.layer.6.attention.self.key.weight\", \"base_model.bert.encoder.layer.6.attention.self.key.bias\", \"base_model.bert.encoder.layer.6.attention.self.value.weight\", \"base_model.bert.encoder.layer.6.attention.self.value.bias\", \"base_model.bert.encoder.layer.6.attention.output.dense.weight\", \"base_model.bert.encoder.layer.6.attention.output.dense.bias\", \"base_model.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.6.intermediate.dense.weight\", \"base_model.bert.encoder.layer.6.intermediate.dense.bias\", \"base_model.bert.encoder.layer.6.output.dense.weight\", \"base_model.bert.encoder.layer.6.output.dense.bias\", \"base_model.bert.encoder.layer.6.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.6.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.7.attention.self.query.weight\", \"base_model.bert.encoder.layer.7.attention.self.query.bias\", \"base_model.bert.encoder.layer.7.attention.self.key.weight\", \"base_model.bert.encoder.layer.7.attention.self.key.bias\", \"base_model.bert.encoder.layer.7.attention.self.value.weight\", \"base_model.bert.encoder.layer.7.attention.self.value.bias\", \"base_model.bert.encoder.layer.7.attention.output.dense.weight\", \"base_model.bert.encoder.layer.7.attention.output.dense.bias\", \"base_model.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.7.intermediate.dense.weight\", \"base_model.bert.encoder.layer.7.intermediate.dense.bias\", \"base_model.bert.encoder.layer.7.output.dense.weight\", \"base_model.bert.encoder.layer.7.output.dense.bias\", \"base_model.bert.encoder.layer.7.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.7.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.8.attention.self.query.weight\", \"base_model.bert.encoder.layer.8.attention.self.query.bias\", \"base_model.bert.encoder.layer.8.attention.self.key.weight\", \"base_model.bert.encoder.layer.8.attention.self.key.bias\", \"base_model.bert.encoder.layer.8.attention.self.value.weight\", \"base_model.bert.encoder.layer.8.attention.self.value.bias\", \"base_model.bert.encoder.layer.8.attention.output.dense.weight\", \"base_model.bert.encoder.layer.8.attention.output.dense.bias\", \"base_model.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.8.intermediate.dense.weight\", \"base_model.bert.encoder.layer.8.intermediate.dense.bias\", \"base_model.bert.encoder.layer.8.output.dense.weight\", \"base_model.bert.encoder.layer.8.output.dense.bias\", \"base_model.bert.encoder.layer.8.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.8.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.9.attention.self.query.weight\", \"base_model.bert.encoder.layer.9.attention.self.query.bias\", \"base_model.bert.encoder.layer.9.attention.self.key.weight\", \"base_model.bert.encoder.layer.9.attention.self.key.bias\", \"base_model.bert.encoder.layer.9.attention.self.value.weight\", \"base_model.bert.encoder.layer.9.attention.self.value.bias\", \"base_model.bert.encoder.layer.9.attention.output.dense.weight\", \"base_model.bert.encoder.layer.9.attention.output.dense.bias\", \"base_model.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.9.intermediate.dense.weight\", \"base_model.bert.encoder.layer.9.intermediate.dense.bias\", \"base_model.bert.encoder.layer.9.output.dense.weight\", \"base_model.bert.encoder.layer.9.output.dense.bias\", \"base_model.bert.encoder.layer.9.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.9.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.10.attention.self.query.weight\", \"base_model.bert.encoder.layer.10.attention.self.query.bias\", \"base_model.bert.encoder.layer.10.attention.self.key.weight\", \"base_model.bert.encoder.layer.10.attention.self.key.bias\", \"base_model.bert.encoder.layer.10.attention.self.value.weight\", \"base_model.bert.encoder.layer.10.attention.self.value.bias\", \"base_model.bert.encoder.layer.10.attention.output.dense.weight\", \"base_model.bert.encoder.layer.10.attention.output.dense.bias\", \"base_model.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.10.intermediate.dense.weight\", \"base_model.bert.encoder.layer.10.intermediate.dense.bias\", \"base_model.bert.encoder.layer.10.output.dense.weight\", \"base_model.bert.encoder.layer.10.output.dense.bias\", \"base_model.bert.encoder.layer.10.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.10.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.11.attention.self.query.weight\", \"base_model.bert.encoder.layer.11.attention.self.query.bias\", \"base_model.bert.encoder.layer.11.attention.self.key.weight\", \"base_model.bert.encoder.layer.11.attention.self.key.bias\", \"base_model.bert.encoder.layer.11.attention.self.value.weight\", \"base_model.bert.encoder.layer.11.attention.self.value.bias\", \"base_model.bert.encoder.layer.11.attention.output.dense.weight\", \"base_model.bert.encoder.layer.11.attention.output.dense.bias\", \"base_model.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"base_model.bert.encoder.layer.11.intermediate.dense.weight\", \"base_model.bert.encoder.layer.11.intermediate.dense.bias\", \"base_model.bert.encoder.layer.11.output.dense.weight\", \"base_model.bert.encoder.layer.11.output.dense.bias\", \"base_model.bert.encoder.layer.11.output.LayerNorm.weight\", \"base_model.bert.encoder.layer.11.output.LayerNorm.bias\", \"base_model.cls.predictions.bias\", \"base_model.cls.predictions.transform.dense.weight\", \"base_model.cls.predictions.transform.dense.bias\", \"base_model.cls.predictions.transform.LayerNorm.weight\", \"base_model.cls.predictions.transform.LayerNorm.bias\", \"base_model.cls.predictions.decoder.weight\", \"base_model.cls.predictions.decoder.bias\". "
     ]
    }
   ],
   "source": [
    "## 搭配\n",
    "texts = [\n",
    "    '女娲有一双聪明能干的手。', \n",
    "    '女娲有一双能干的手。', \n",
    "    '女娲有一双聪明的手。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张强从小生活在爷爷奶奶身边，因而对父母有着浓厚的感情。\n",
      "[]\n",
      "张强从小生活在爷爷奶奶身边，却对父母有着浓厚的感情。\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '张强从小生活在爷爷奶奶身边，因而对父母有着浓厚的感情。', \n",
    "    '张强从小生活在爷爷奶奶身边，却对父母有着浓厚的感情。',\n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们不仅要在课外学语文，还要在课堂中学语文。\n",
      "[]\n",
      "我们不仅要在课堂中学语文，还要在课外学语文。\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '我们不仅要在课外学语文，还要在课堂中学语文。', \n",
    "    '我们不仅要在课堂中学语文，还要在课外学语文。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熟悉小华的人都知道，他在生活中并不是一个活泼开朗的人。\n",
      "[]\n",
      "熟悉小华的人都知道，他在生活中并不是一个活泼开朗的人，\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '熟悉小华的人都知道，他在生活中并不是一个活泼开朗的人。', \n",
    "    '熟悉小华的人都知道，他在生活中并不是一个活泼开朗的人，', \n",
    "]\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明待人非常大方友善得很。\n",
      "['[CLS]' '小' '明' '待' '非' '常' '大' '方' '友' '得' '很' '。' '[SEP]']\n",
      "小明待人非常大方友善。\n",
      "['[CLS]' '小' '明' '非' '常' '。' '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '小明待人非常大方友善得很。', \n",
    "    '小明待人非常大方友善。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他为了民族的兴亡和人民的利益奋斗了一生。\n",
      "[]\n",
      "他为了民族的复兴和人民的利益奋斗了一生。\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 语义：单面对双面\n",
    "texts = [\n",
    "    '他为了民族的兴亡和人民的利益奋斗了一生。', \n",
    "    '他为了民族的复兴和人民的利益奋斗了一生。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不禁怀疑这条题目是不是老师讲错了。\n",
      "[]\n",
      "我不禁怀疑这道题目是老师讲错了。\n",
      "['是']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '我不禁怀疑这条题目是不是老师讲错了。', \n",
    "    '我不禁怀疑这道题目是老师讲错了。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_char = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "面对经济不断下行的形势，政府采取了一系列措施来刺激消费，以促进经济繁荣和复苏。\n",
      "[]\n",
      "面对经济不断下行，政府采取了一系列措施来刺激消费，以促进经济繁荣和复苏。\n",
      "['面' '对' '不' '断' '下' '行']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '面对经济不断下行的形势，政府采取了一系列措施来刺激消费，以促进经济繁荣和复苏。', \n",
    "    '面对经济不断下行，政府采取了一系列措施来刺激消费，以促进经济繁荣和复苏。'\n",
    "]\n",
    "probs, seq_probs, err_char = apply_ged_pipeline(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从小明这一个简单的举动中，可以让人看出他是一个细心的好孩子。\n",
      "['从' '这' '中' '，' '可' '以' '让']\n",
      "小明这一个简单的举动，可以让人看出他是一个细心的好孩子。\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我想，人是由三部分组成的：对往事的追忆、对未来的憧憬和对现时的把握。\n",
      "[]\n",
      "我想，人是由三部分组成的：对往事的追忆、对现时的把握和对未来的憧憬。\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '从小明这一个简单的举动中，可以让人看出他是一个细心的好孩子。', \n",
    "    '小明这一个简单的举动，可以让人看出他是一个细心的好孩子。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_tokens = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)\n",
    "\n",
    "texts = [\n",
    "    '我想，人是由三部分组成的：对往事的追忆、对未来的憧憬和对现时的把握。', \n",
    "    '我想，人是由三部分组成的：对往事的追忆、对现时的把握和对未来的憧憬。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_tokens = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时光的流逝不能让我淡去对故乡浓浓的思念，反之，随着年龄的增长，对故乡的思念愈发日久弥坚。\n",
      "['[CLS]' '时' '的' '流' '不' '能' '让' '我' '淡' '去' '对' '故' '乡' '浓' '浓' '的' '思'\n",
      " '念' '，' '反' '之' '，' '随' '着' '年' '龄' '的' '增' '长' '，' '对' '故' '乡' '的' '思'\n",
      " '念' '愈' '发' '日' '久' '弥' '坚' '。' '[SEP]']\n",
      "时光的流逝不能让我淡去对故乡浓浓的思念，反之，随着年龄的增长，对故乡的思念日久弥坚。\n",
      "['[CLS]' '不' '能' '我' '，' '反' '之' '，' '随' '着' '的' '增' '长' '，' '对' '的' '念'\n",
      " '日' '久' '弥' '坚' '。' '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 成分残缺/赘余\n",
    "\n",
    "texts = [\n",
    "    '时光的流逝不能让我淡去对故乡浓浓的思念，反之，随着年龄的增长，对故乡的思念愈发日久弥坚。', \n",
    "    '时光的流逝不能让我淡去对故乡浓浓的思念，反之，随着年龄的增长，对故乡的思念日久弥坚。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_tokens = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朱悦报了一个培训班，她固执的思想却依然没有改变。\n",
      "['报' '了']\n",
      "朱悦报了一个培训班，她固执的思想却依然没有改变。\n",
      "['报' '了']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '朱悦报了一个培训班，她固执的思想却依然没有改变。', \n",
    "    '朱悦报了一个培训班，她固执的思想却依然没有改变。', \n",
    "]\n",
    "\n",
    "probs, seq_probs, err_tokens = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下午三点整，参加典礼的群众全部到齐了。\n",
      "[]\n",
      "下午三点整，参加典礼的群众到齐了。\n",
      "[]\n",
      "下午三点整，参加典礼的群众全部到了。\n",
      "['，' '的' '全' '部' '到' '了' '。']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '下午三点整，参加典礼的群众全部到齐了。',\n",
    "     '下午三点整，参加典礼的群众到齐了。',\n",
    "     '下午三点整，参加典礼的群众全部到了。',\n",
    "]\n",
    "\n",
    "probs, seq_probs, err_tokens = clf(texts=texts, checkpoints=checkpoints, device=device, output_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\chinese-grammar-error-detection\\notebooks\\..\\dataset.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indexed_value = torch.tensor(value[index]).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "山上的水很宝贵，我们把它留给晚上来的人喝。\n",
      "['[CLS]' '的' '，' '我' '们' '把' '它' '留' '给' '晚' '上' '来' '的' '人' '喝' '。'\n",
      " '[SEP]']\n",
      "中央经济工作会议是制定第二年宏观经济政策、判断当前经济形势最权威的风向标，也是每年度级别最高的经济工作会议。\n",
      "['每' '年' '度' '级' '别' '最']\n",
      "到明年，我省将形成开发、销售、生产、检测、服务为一体的新能源汽车产业发展体系。\n",
      "['开' '发' '销' '售' '产']\n",
      "世界知识产权组织表示，高度发达经济体一直占据在全球创新指数中的主导地位，中国进入25强标志着中等收入国家首次进入高度发达经济体行列。\n",
      "[]\n",
      "庄子告诉我们，境界决定了人们对事物判断的正误，站在大境界上，就会看到天生我材必有用，而站在小境界上只能一生碌碌无为。\n",
      "['看' '到' '必']\n",
      "古希腊时代之所以能创造出维纳斯、持矛者、掷铁饼者这些千古不朽的雕塑，是由于艺术家对人的完美形体有一种衷心的迷恋。\n",
      "[]\n",
      "该剧的意义在于，它以生动的画面和理念展示了新时期我军训练和生活的方方面面，揭开了新世纪“新军事题材电视剧”创作的序幕。\n",
      "[]\n",
      "他那无私的精神是我们学习的榜样。\n",
      "['他' '那']\n",
      "萨马兰奇2010年4月21日离开人世，这位老人曾为邓亚萍永不服输的精神而打动，他表示，邓亚萍的非凡成绩是她的天分、艰苦努力和不屈不挠的精神结合的结果，体现了奥运精神。\n",
      "[]\n",
      "珠三角地区作为国家实施“一带一路”战略的重要支点，贵广、南广高铁的开通，将使贵州等西南地区更广泛地融入“一带一路”之中。\n",
      "[]\n",
      "个人之所以成其为个人，以及他的生存之所以有意义，与其说是靠他个人的力量，也是由于他是人类社会的一个成员，社会在支配着他的物质生活和精神生活。\n",
      "[]\n",
      "最近，北京的社区除了食品、药店和日杂零售外，又出现了一些家居品牌社区店。\n",
      "['除' '了' '、' '日' '零' '售' '家']\n",
      "这家伙相当顽强，死也不肯坦白。\n",
      "['这' '相' '当' '顽' '，' '死' '也']\n",
      "食品添加剂的使用标准包括食品用加工助剂、胶母糖基础剂和食品用香料等2314个品种。\n",
      "['[CLS]' '食' '品' '添' '加' '剂' '的' '使' '用' '标' '准' '包' '括' '食' '品' '用' '加'\n",
      " '工' '助' '剂' '、' '胶' '母' '糖' '基' '础' '剂' '和' '食' '品' '用' '香' '料' '等' '2'\n",
      " '3' '1' '4' '个' '品' '种' '。' '[SEP]']\n",
      "整座大桥在河中的部分没有一个桥墩，桥身全靠铁索拉起，这在国内还是先例。\n",
      "['在' '河' '的' '部' '分' '，' '这' '在' '还' '是' '先' '例' '。']\n",
      "由青年导演周申执导，开心麻花团队制作的影片《驴得水》，是一部2016年难得的兼具商业性和艺术性的现实主义荒诞喜剧片。\n",
      "['0' '6' '年']\n",
      "《国家通用语言文字法》的实施、颁布是我国语文生活中的一件大事，标志着我国语言文字规范化、标准化工作开始走上法制轨道，进入一个新的发展时期。一项数据显示，大约2%以上的普通人和40%~50%的持续性哮喘儿童对猫过敏，且对猫过敏的人或是狗的两倍。\n",
      "['。' '，' '大' '约' '%' '以' '上' '的' '和' '0' '%' '0' '续' '哮' '喘' '儿' '过' '敏'\n",
      " '，' '过' '敏' '的' '人' '或' '是' '狗' '的' '两' '。' '[SEP]']\n",
      "“十八大”报告中指出的“既不走封闭僵化的老路、也不走改旗易帜的邪路”，这是中国共产党人的坚定信念，是对当今世界格局、人类社会发展史的准确把握。\n",
      "[]\n",
      "4月27日，朝韩两国领导人在板门店会晒，为朝鲜半岛实现和平迈出了弥足珍贵的一步。苹果公司不仅为广大用户提供创新产品及相关解决方案，更重视用户的应用体验，凭借专业精深的技术帮助用户加速采用以及有效使用苹果系列产品。\n",
      "['[CLS]' '步' '。' '苹' '果' '公' '司' '不' '仅' '为' '广' '大' '用' '户' '提' '供' '创'\n",
      " '新' '产' '品' '及' '相' '关' '解' '决' '方' '案' '，' '更' '重' '视' '用' '户' '的' '用'\n",
      " '体' '验' '，' '凭' '借' '专' '业' '精' '深' '的' '技' '术' '帮' '助' '用' '户' '加' '速'\n",
      " '采' '用' '以' '及' '有' '效' '使' '用' '苹' '果' '系' '列' '产' '品' '。' '[SEP]']\n",
      "全国国民阅读调查结果显示，国民人均纸质图书阅读量为4．77本，人均阅读电子书2．48本，超五成的成年国民认为自己的阅读数量较少。\n",
      "[]\n",
      "人们在生活中，实际上只需要遵守那些最基本的规则，而这些规则在幼儿园里就学过。\n",
      "[]\n",
      "[[0.61519563 0.38480437]\n",
      " [0.0621442  0.9378558 ]\n",
      " [0.14664641 0.85335356]\n",
      " [0.33251962 0.66748035]\n",
      " [0.8807901  0.11920984]\n",
      " [0.2520167  0.74798334]\n",
      " [0.8502855  0.14971448]\n",
      " [0.36574158 0.63425845]\n",
      " [0.22913225 0.7708677 ]\n",
      " [0.75866914 0.24133086]\n",
      " [0.42430508 0.5756949 ]\n",
      " [0.5808859  0.41911408]\n",
      " [0.21183448 0.7881655 ]\n",
      " [0.23371348 0.76628655]\n",
      " [0.01816397 0.9818361 ]\n",
      " [0.13886744 0.8611325 ]\n",
      " [0.26099724 0.73900276]\n",
      " [0.11598005 0.8840199 ]\n",
      " [0.8340351  0.16596495]\n",
      " [0.05152903 0.94847095]\n",
      " [0.94532853 0.05467153]\n",
      " [0.82543147 0.17456856]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '', \n",
    "    # '修建青藏铁路是加快西部大开发的重要举措，是民族团结的重要纽带。',\n",
    "    # '《国家宝藏》用镜头带领观众走进博物馆，力图对每一件文物的前世今生进行总结与梳理，让观众在一眼万年中，感悟传统文化的深厚魅力。',\n",
    "    # '中美双方希望通过此次访问增进互信、扩大合作、加强对话，推动中美关系健康稳定地向前发展。',\n",
    "    # '微笑是一首动人的歌，它让我们的生活充满温馨。',\n",
    "    # '上海科技开发中心聚集了一批热心于科技开发服务、善于经营管理的专业化队伍。',\n",
    "    # '原疾控中心副主任杨功焕认为，如现有防控措施得到较好执行，疫情应该可以控制。',\n",
    "    # '本协议自1985年1月1日起至1990年12月31日止，有效期5年，其间如无特殊原因，双方均有恪守协议的义务。',\n",
    "    # '很多城市都有其独特的城市记忆，相关部门不能简单地用“有没有住过名人”、“有没有发生过著名事件”作为拆除老建筑的依据。',\n",
    "    # '分小班能针对性地解决学生存在的各种问题。',\n",
    "    # '2011年利润排名前40家企业，瓜分了6000多亿央企利润中的95%，其中有12家企业利润超过了100亿，这“十二豪门”囊括了央企总利润的78.8%。',\n",
    "    # '九部门强调，严禁任何单位和个人在食品生产经营中使用食品添加剂以外的任何化学物质，严禁在农产品种植、加工、收购、运输中使用违禁药物或其他可能危害人体健康的物质。',\n",
    "    # '《国家社科基金项目2010年度课题指南》近日发布，指南强调，基础理论研究要力求具有开拓性和原创性，应用对策研究要力求具有针对性和现实性，着力推出代表国家水平的哲学社会科学研究成果。', \n",
    "    # '诗歌中的意象是诗人所写的景与物，但这些意象又不再是单纯的景和物了，而是融入了诗人的情感，这些独特的物象实际上就是作者的外化。', \n",
    "    # '上海世博将充分交流城市建设经验和先进城市发展理念，探讨新的更好的人类居住、生活和工作的模式，为人类可持续发展留下一份丰厚的精神遗产。', \n",
    "    # '我们被统一的标准时间绑架住了，就像我们现在很多人的生活被北上广的时间和生活所绑架了一样，因此我们需要回到时间本身。',\n",
    "    # '党的历史表明，群众路线执行得好坏，关系着革命和建设事业的成功。' , \n",
    "    # '女娲有一双聪明能干的手。', \n",
    "    # '大凡世界上的人物多是复杂的，好文章多是婉曲的，所以，我们的眼光也应婉曲、复杂些。', \n",
    "    # '1827年，雨果发表了《<克伦威尔>序言》，主张自然中存在的一切都可成为艺术题材，并且提出了美丑对照的审美原则，从而成为浪漫主义文学运动的宣言。', \n",
    "    # '为加强酒类的专卖管理，某市规定，凡是从外地订购的白酒，必须经市（县）专卖事业管理局批准，领取准购证后方得进货。', \n",
    "    # '全球有6700万人患有自闭症，而在中国，这个数字是1000万。虽然身边有着这么多的自闭症患者，但很多人对自闭症还是一个陌生的概念。', \n",
    "    # '亚洲文明对话大会不仅为世界文明发展指明了方向，而且促进了亚洲各国间的平等对话。', \n",
    "    # '荣获了“感动中国十大人物”的张艺谋在接受采访时，始终将自己的团队挂在嘴边，强调真正伟大的不是自己，而是自己的团队，自己的国家。',\n",
    "    # '肿瘤专家认为吸烟或吸二手烟是引起肺癌的第一大诱因，而中国青少年吸烟率正在逐年上升，因此，教育部新近颁布的校园禁烟令受到了社会的肯定。',  \n",
    "    # '对这家企业来说，现在要做的就是整顿，以度过当前的危机，让消费者重建对产品的信任。我想，这个过程是艰巨而漫长的。', \n",
    "    # '更多书刊将政策的善意倾注笔端，以此疏导社会情绪、纾解时代焦虑，决定着同心同德的发展共识是否会串联起亿万人民对于国家未来的憧憬和期待。', \n",
    "    # '艾青继承了中国古代知识分子“以天下为己任”的忧患意识，使他始终具有时代的使命感和忧国忧民的情怀。', \n",
    "    # '全球两个最大碳排放国的承诺，使过去五年五次全球气候谈判大会试图谈成的事，给谈成了。', \n",
    "    # '新的土地法规定，农民耕种的符合政策规定的自留地是一种正当的劳动，各级政府不得以各种理由加以干涉。', \n",
    "    # '投资环境的好坏，服务质量的优劣，政府公务员素质的高低，都是新浦新区经济健康发展的重要保证。', \n",
    "    # '规规矩矩的两条平行线，始终是两个可望而不可及的端点。', \n",
    "    # '象征中华民族5000年文明的考古证据在辽河流域一再被发现，证明中华文明起源不是一个中心而是多个中心，其中包括辽河流域在内的燕山南北长城地带也是中华文明的发祥地之一。', \n",
    "    # '他为了民族的兴亡和人民的利益奋斗了一生。', \n",
    "    # '熊猫贝贝这种惊人的生长速度，是和食物的充分供应以及每天喂食时间的长短成正比的。', \n",
    "    # '一个人能取得卓越的成就，并不在于他就读的学校是重点还是普通,而在于他是否具备成功的特质。', \n",
    "    # '这次环保督查“回头看”，是为了有效纠正处分执行得是否到位的问题，真正发挥处分应有的警示、惩戒作用。',\n",
    "    # '中国水稻专家说，凭借世界领先的超级杂交稻技术，13亿中国人不但可以吃得好，而且可以吃得饱。', \n",
    "    # '我们严肃地研究了职工们的建议。又虚心地征求了专家们的意见。',  \n",
    "    # '这次比赛的获胜，将决定我们能否进入决赛阶段。', \n",
    "    # '时光的流逝不能让我淡去对故乡浓浓的思念，反之，随着年龄的增长，对故乡的思念愈发日久弥坚。'\n",
    "    # '以损人利己手段牟取财富的，无论多少，都是肮脏的；而损人利己的致富者应视为“社会公敌”。', \n",
    "    # '《断章》只有短短4行，却试图用简明的意象阐释道理的深刻。诗人通过对“风景”的刹那间的感悟，涉及了“相对性”的哲理命题。', \n",
    "    # '在课堂教学中培养学生创新精神，是课堂教学改革成败的关键。', \n",
    "    # '面对这种保安员随意搜身的现象，人们不禁要问：他们的职责是保护商场的货物安全和环境安全，不是执法机构，怎么能擅自对顾客检查呢？', \n",
    "    # '希腊公共秩序部部长表示，希腊政府已经动用了反恐法并出动反恐部队来搜寻纵火犯，这意味着政府开始更加激进的措施应对国内森林大火。', \n",
    "    # '从无到有，中国航母出现在东方的海平面上。从试航、改装到正式入列，“辽宁舰”迈出的这一步注定是中国航母从梦想走向现实的一大步。', \n",
    "    # '许多学校的班级都参加了这次感恩节活动。', \n",
    "    # '今年辽宁旅游将以做精旅游产品、做强旅游企业、做好旅游服务为重点，推进旅游产业转型升级。', \n",
    "    # '住房公积金是指国家机关、企事业单位、外商投资企业、城镇私营企业及其他城镇企业及其在职职工缴存的长期住房储金，具有强制性。', \n",
    "    # '对一个国家而言，从宣示主权到管辖疆域，从资源勘探到建设规划，标准规范的地图不仅关系到国家安全、民族尊严，还反映着领土轮廓，作用重大，意义非凡。',\n",
    "    # '2020年6月23日，我国成功发射了北斗三号最后一颗全球组网卫星，具备短报文能力，可以传送图像、打语音电话。', \n",
    "    # '路瓦栽夫人绕着脖子把项链挂在她那长长的高领上，站在镜子前对着自己的影子出神好半天。', \n",
    "    # '回首三年的初中生活，我们再一次明确了这样的道理：勤奋、踏实是学习成败的关键。', \n",
    "    # '而本设计所要求表现的美是野草之美，平常之美，那些被践踏、被遗忘、被鄙视的自然的美。', \n",
    "    # '屈原在《湘夫人》中表现了男女水神欢会难期、思而不见的爱情悲剧，实际上是他自己不为楚王所知的身世悲剧的曲折反映。', \n",
    "    # '沈从文创作的小说主要以湘西生活和都市生活为题材。前者通过都市生活的腐化堕落，揭示都市自然人性的丧失；后者通过描写湘西人原始、自然的生命形式，赞美人性美。', \n",
    "    # '9月15日，“天宫二号”成功发射，这不但为中国在2020年前后建成永久性空间站打下基础，而且使我国拥有了真正意义上的太空实验室。', \n",
    "    '山上的水很宝贵，我们把它留给晚上来的人喝。', \n",
    "    '中央经济工作会议是制定第二年宏观经济政策、判断当前经济形势最权威的风向标，也是每年度级别最高的经济工作会议。', \n",
    "    '到明年，我省将形成开发、销售、生产、检测、服务为一体的新能源汽车产业发展体系。', \n",
    "    '世界知识产权组织表示，高度发达经济体一直占据在全球创新指数中的主导地位，中国进入25强标志着中等收入国家首次进入高度发达经济体行列。', \n",
    "    '庄子告诉我们，境界决定了人们对事物判断的正误，站在大境界上，就会看到天生我材必有用，而站在小境界上只能一生碌碌无为。', \n",
    "    '古希腊时代之所以能创造出维纳斯、持矛者、掷铁饼者这些千古不朽的雕塑，是由于艺术家对人的完美形体有一种衷心的迷恋。', \n",
    "    '该剧的意义在于，它以生动的画面和理念展示了新时期我军训练和生活的方方面面，揭开了新世纪“新军事题材电视剧”创作的序幕。', \n",
    "    '他那无私的精神是我们学习的榜样。', \n",
    "    '萨马兰奇2010年4月21日离开人世，这位老人曾为邓亚萍永不服输的精神而打动，他表示，邓亚萍的非凡成绩是她的天分、艰苦努力和不屈不挠的精神结合的结果，体现了奥运精神。', \n",
    "    '珠三角地区作为国家实施“一带一路”战略的重要支点，贵广、南广高铁的开通，将使贵州等西南地区更广泛地融入“一带一路”之中。', \n",
    "    '个人之所以成其为个人，以及他的生存之所以有意义，与其说是靠他个人的力量，也是由于他是人类社会的一个成员，社会在支配着他的物质生活和精神生活。', \n",
    "    '最近，北京的社区除了食品、药店和日杂零售外，又出现了一些家居品牌社区店。',\n",
    "    '这家伙相当顽强，死也不肯坦白。', \n",
    "    '食品添加剂的使用标准包括食品用加工助剂、胶母糖基础剂和食品用香料等2314个品种。', \n",
    "    '整座大桥在河中的部分没有一个桥墩，桥身全靠铁索拉起，这在国内还是先例。', \n",
    "    '由青年导演周申执导，开心麻花团队制作的影片《驴得水》，是一部2016年难得的兼具商业性和艺术性的现实主义荒诞喜剧片。', \n",
    "    '《国家通用语言文字法》的实施、颁布是我国语文生活中的一件大事，标志着我国语言文字规范化、标准化工作开始走上法制轨道，进入一个新的发展时期。'\n",
    "    '一项数据显示，大约2%以上的普通人和40%~50%的持续性哮喘儿童对猫过敏，且对猫过敏的人或是狗的两倍。', \n",
    "    '“十八大”报告中指出的“既不走封闭僵化的老路、也不走改旗易帜的邪路”，这是中国共产党人的坚定信念，是对当今世界格局、人类社会发展史的准确把握。', \n",
    "    '4月27日，朝韩两国领导人在板门店会晒，为朝鲜半岛实现和平迈出了弥足珍贵的一步。'\n",
    "    '苹果公司不仅为广大用户提供创新产品及相关解决方案，更重视用户的应用体验，凭借专业精深的技术帮助用户加速采用以及有效使用苹果系列产品。', \n",
    "    '全国国民阅读调查结果显示，国民人均纸质图书阅读量为4．77本，人均阅读电子书2．48本，超五成的成年国民认为自己的阅读数量较少。', \n",
    "    '人们在生活中，实际上只需要遵守那些最基本的规则，而这些规则在幼儿园里就学过。', \n",
    "]\n",
    "scores, _, _ = apply_ged_pipeline(texts)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "664321c82ff6de4bb3d6cb89c025cd05a28d5519bb13940eaa668e3035d94110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
