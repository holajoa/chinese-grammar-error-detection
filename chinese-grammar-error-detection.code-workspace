{
	"folders": [
		{
			"path": "."
		}
	],
	"settings": {
		"python.defaultInterpreterPath": "D:/Apps/Anaconda3/envs/general-torch/python.exe",
		"python.autoComplete.extraPaths": [
			"D:/Develop/chinese-grammar-error-detection", 
		],
		"python.analysis.extraPaths": [
			"D:/Develop/chinese-grammar-error-detection"
		]
	},
	"launch": {
		"version": "0.2.0",
		"configurations": [
			{
				"name": "notebooks",
				"type": "python",
				"request": "launch",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": false, 
				"args":[]
			}, 
			{
				"name": "Ernie-token-level-finetune",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "hfl/chinese-macbert-base", 
					"--num_labels", "2",  
					"--data_dir", "data/data-aug-trunc",  
					"--maxlength", "64",  
					"--pred_output_dir", "submissions-ernie-ft" , 
					"--output_model_dir", "finetuned_models/ernie_finetune", 
					"--epoch", "2",  
					"--batch_size", "64",  
					"--kfolds", "10",
					"--perform_testing", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "1", 
					"--perform_testing",
					"--single_layer_cls",
					"--token_level_model", 
					"--easy_ensemble", 
					"--resume_fold_idx",  "1",  
					"--checkpoint",  "finetuned_models/balanced_trial_ernie_gram/fold1/checkpoint-1910/pytorch_model.bin", 
					"--from_another_run", 
				]
			}, 
			{
				"name": "Cleaned up main.py",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "uer/roberta-base-word-chinese-cluecorpussmall", 
					"--num_labels", "2", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/word-based-roberta", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "5", 
					"--batch_size", "32", 
					"--lr", "1e-5", 
					"--alpha", "1.25", 
					"--gamma", "2", "", "", 
					"--pooling_mode", "hybrid", 
					"--local_loss_param", "1e-3", 
					"--early_stopping_patience", "3", 
					"--best_by_f1", "", 
					"--perform_testing", 
				]
			}, 
			{
				"name": "with oob model",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run-oob.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "hfl/chinese-macbert-base",
					"--oob_model_name", "KoichiYasuoka/chinese-bert-wwm-ext-upos",  
					"--num_labels", "2", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/macbert_with_wwm_upos", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "10", 
					"--batch_size", "64", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "2", 
					"--early_stopping_patience", "4", 
					"--best_by_f1", 
					"--perform_testing", 
				]
			}, 
			{
				"name": "with oob model resume",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run-oob.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "hfl/chinese-macbert-base",
					"--oob_model_name", "KoichiYasuoka/chinese-bert-wwm-ext-upos",  
					"--num_labels", "2", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--folds",  "finetuned_models/macbert_with_wwm_upos/data/folds.txt",
    				"--resume_model_idx", "4", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/macbert_with_wwm_upos", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "10", 
					"--batch_size", "64", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "2", 
					"--early_stopping_patience", "4", 
					"--best_by_f1", 
					"--perform_testing", 
				]
			}, 
			{
				"name": "generated data test",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run-oob.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "hfl/chinese-macbert-base", 
					"--oob_model_name", "KoichiYasuoka/chinese-bert-wwm-ext-upos", 
					"--num_labels", "2", 
					"--data_dir", "data/data-gen/expanded", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--minor_major_ratio", "0.75", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/gen_data", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "5", 
					"--batch_size", "16", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "4", 
					"--early_stopping_patience", "4", 
					"--perform_testing", 
					"--resume_model_idx", "4"
				]
			}, 
			{
				"name": "ensemble model",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "uer/roberta-base-word-chinese-cluecorpussmall", 
					"--base_model_checkpoint", "finetuned_models/ww-baseline/checkpoint-3882", 
					"--num_labels", "2", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--minor_major_ratio", "0.5", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/rww-mini", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "10", 
					"--batch_size", "32", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "2", 
					"--early_stopping_patience", "3", 
					"--do_pred_on_dev_set", 
					"--num_training_examples", "500"
				]
			}, 
			{
				"name": "bigru",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/run.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "uer/roberta-base-word-chinese-cluecorpussmall", 
					"--num_labels", "2", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--kfolds", "10", 
					"--minor_major_ratio", "0.5", 
					"--pred_output_dir", "submissions", 
					"--output_model_dir", "finetuned_models/rww-bigru", 
					"--n_fold_used", "5", 
					"--num_ensemble_models", "5", 
					"--num_epochs", "10", 
					"--batch_size", "32", 
					"--lr", "2e-5", 
					"--alpha", "1", 
					"--gamma", "2", 
					"--early_stopping_patience", "3", 
					"--do_pred_on_dev_set", 
					"--bigru"
				]
			}, 
			{
				"name": "mlm-finetune",
				"type": "python",
				"request": "launch",
				"program": "D:/Develop/chinese-grammar-error-detection/mlm-finetune.py",
				"python": "D:/Apps/Anaconda3/envs/general-torch/python.exe", 
				"console": "integratedTerminal",
				"justMyCode": true, 
				"args":[
					"--model_name", "uer/roberta-base-word-chinese-cluecorpussmall", 
					"--data_dir", "data/data-org", 
					"--maxlength", "64", 
					"--output_model_dir", "finetuned_models/ww-baseline", 
					"--num_epochs", "8", 
					"--batch_size", "16", 
					"--lr", "3e-5", 
				]
			}


		]
	}
}