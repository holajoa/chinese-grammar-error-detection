(general-torch) PS D:\Develop\chinese-grammar-error-detection> sh shell\train-ner.sh
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.837, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.5852, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.2933, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.1295, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [21:47<57:48,  2.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.901109218597412, 'eval_F1': 0.8720499768625637, 'eval_precision': 0.8595210946408209, 'eval_recall': 0.8849495186663536, 'eval_runtime': 42.8627, 'eval_samples_per_second': 131.956, 'eval_steps_per_second': 8.259, 'epoch': 1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:30<57:48,  2.14it/s]Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-2475\special_tokens_map.json
{'loss': 2.9218, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.3254, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.2545, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.1836, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.2806, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:33<39:52,  2.07it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.807204008102417, 'eval_F1': 0.8581772784019975, 'eval_precision': 0.9162889896027726, 'eval_recall': 0.8069969476402912, 'eval_runtime': 43.0782, 'eval_samples_per_second': 131.296, 'eval_steps_per_second': 8.218, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:17<39:52,  2.07it/s]Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-4950\special_tokens_map.json
{'loss': 2.1224, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3668, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.4014, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.3379, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.4857, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:17<19:43,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.7970082759857178, 'eval_F1': 0.868917576961271, 'eval_precision': 0.9217803529101922, 'eval_recall': 0.8217891523831885, 'eval_runtime': 43.0903, 'eval_samples_per_second': 131.259, 'eval_steps_per_second': 8.215, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:00<19:43,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-7425\special_tokens_map.json
{'loss': 1.2631, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.9795, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.2077, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.0801, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.9296, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:01<00:00,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.001411437988281, 'eval_F1': 0.8718645743234894, 'eval_precision': 0.92018779342723, 'eval_recall': 0.828363465602254, 'eval_runtime': 42.9838, 'eval_samples_per_second': 131.584, 'eval_steps_per_second': 8.236, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:44<00:00,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold0\checkpoint-2475 (score: 0.8720499768625637).
{'train_runtime': 5447.2634, 'train_samples_per_second': 29.073, 'train_steps_per_second': 1.817, 'train_loss': 2.009107684511127, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:47<00:00,  1.82it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.29it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.41it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.7298, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.4904, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.2659, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.0575, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [22:00<56:36,  2.19it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.8368072509765625, 'eval_F1': 0.8220197613242655, 'eval_precision': 0.894442893046635, 'eval_recall': 0.7604463437796771, 'eval_runtime': 41.7063, 'eval_samples_per_second': 135.615, 'eval_steps_per_second': 8.488, 'epoch': 1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:42<56:36,  2.19it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-2475\special_tokens_map.json
{'loss': 2.9344, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.1128, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.1633, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.1613, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.1205, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:46<40:52,  2.02it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.9759902954101562, 'eval_F1': 0.8276314064752917, 'eval_precision': 0.9239098624524437, 'eval_recall': 0.7495251661918328, 'eval_runtime': 41.3945, 'eval_samples_per_second': 136.637, 'eval_steps_per_second': 8.552, 'epoch':2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:27<40:52,  2.02it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-4950\special_tokens_map.json
{'loss': 1.8751, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3468, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.2808, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.292, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.3572, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:18<19:29,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.22636079788208, 'eval_F1': 0.8592927012791572, 'eval_precision': 0.9106858054226475, 'eval_recall': 0.8133903133903134, 'eval_runtime': 43.1992, 'eval_samples_per_second': 130.928, 'eval_steps_per_second': 8.195, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:01<19:29,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-7425\special_tokens_map.json
{'loss': 1.2254, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.9772, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.091, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.0383, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.0354, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:02<00:00,  2.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.507601261138916, 'eval_F1': 0.8722802704363859, 'eval_precision': 0.9044098903900076, 'eval_recall': 0.842355175688509, 'eval_runtime': 43.0213, 'eval_samples_per_second': 131.47, 'eval_steps_per_second': 8.228, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:45<00:00,  2.15it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold1\checkpoint-9900 (score: 0.8722802704363859).
{'train_runtime': 5448.4021, 'train_samples_per_second': 29.067, 'train_steps_per_second': 1.817, 'train_loss': 1.9360305570351957, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:48<00:00,  1.82it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.28it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.38it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.8658, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.5431, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.2491, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.1199, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [21:57<58:24,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.8505969047546387, 'eval_F1': 0.8181818181818182, 'eval_precision': 0.899488926746167, 'eval_recall': 0.7503552818569399, 'eval_runtime': 43.0504, 'eval_samples_per_second': 131.381, 'eval_steps_per_second': 8.223, 'epoch': 1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:40<58:24,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-2475\special_tokens_map.json
{'loss': 2.9505, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.2678, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.1614, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.1561, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.1523, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:35<38:25,  2.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.118971109390259, 'eval_F1': 0.8481806775407779, 'eval_precision': 0.9018143009605123, 'eval_recall': 0.8005684509711037, 'eval_runtime': 43.109, 'eval_samples_per_second': 131.202, 'eval_steps_per_second': 8.212, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:18<38:25,  2.15it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-4950\special_tokens_map.json
{'loss': 2.0604, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3619, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.3423, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.3405, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.3455, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:15<19:24,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.4883880615234375, 'eval_F1': 0.8696186961869619, 'eval_precision': 0.9045547594677584, 'eval_recall': 0.8372809095215538, 'eval_runtime': 43.0294, 'eval_samples_per_second': 131.445, 'eval_steps_per_second': 8.227, 'epoch':3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:58<19:24,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-7425\special_tokens_map.json
{'loss': 1.2585, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 1.0039, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.0978, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.1452, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.0271, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:58:17<00:00,  2.41it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.347602367401123, 'eval_F1': 0.8660459342023589, 'eval_precision': 0.909992173232455, 'eval_recall': 0.8261487446707722, 'eval_runtime': 33.1988, 'eval_samples_per_second': 170.367, 'eval_steps_per_second': 10.663, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:58:50<00:00,  2.41it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold2\checkpoint-7425 (score: 0.8696186961869619).
{'train_runtime': 7133.8478, 'train_samples_per_second': 22.2, 'train_steps_per_second': 1.388, 'train_loss': 1.9823333586105192, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:58:53<00:00,  1.39it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:33<00:00, 10.65it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:06<00:00, 10.82it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.7362, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.4721, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.2205, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.0125, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████                                                         | 2475/9900 [21:46<1:00:13,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.8028881549835205, 'eval_F1': 0.8055146566724669, 'eval_precision': 0.921875, 'eval_recall': 0.7152365105776087, 'eval_runtime': 42.8357, 'eval_samples_per_second': 132.04, 'eval_steps_per_second': 8.264, 'epoch': 1.0}
 25%|███████████████████                                                         | 2475/9900 [22:29<1:00:13,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-2475\special_tokens_map.json
{'loss': 2.86, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.1466, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.0617, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.0705, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.0106, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:21<38:14,  2.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.099553346633911, 'eval_F1': 0.8704663212435233, 'eval_precision': 0.9048473967684022, 'eval_recall': 0.8386023294509152, 'eval_runtime': 42.9714, 'eval_samples_per_second': 131.622, 'eval_steps_per_second': 8.238, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:04<38:14,  2.16it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-4950\special_tokens_map.json
{'loss': 1.9083, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3415, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.3033, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.3043, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.3489, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:06:54<19:21,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.8985705375671387, 'eval_F1': 0.859394863971523, 'eval_precision': 0.9237496583766056, 'eval_recall': 0.8034228666508201, 'eval_runtime': 42.8711, 'eval_samples_per_second': 131.93, 'eval_steps_per_second': 8.257, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:37<19:21,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-7425\special_tokens_map.json
{'loss': 1.1691, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.9577, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.1412, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.9824, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.0289, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:29:27<00:00,  2.08it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.116557598114014, 'eval_F1': 0.8729309271935283, 'eval_precision': 0.9161442006269592, 'eval_recall': 0.8336106489184693, 'eval_runtime': 42.9509, 'eval_samples_per_second': 131.685, 'eval_steps_per_second': 8.242, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:10<00:00,  2.08it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold3\checkpoint-9900 (score: 0.8729309271935283).
{'train_runtime': 5413.0592, 'train_samples_per_second': 29.257, 'train_steps_per_second': 1.829, 'train_loss': 1.9114419247405698, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:13<00:00,  1.83it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.31it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.42it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.8765, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.867, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.8143, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.4751, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [21:52<58:57,  2.10it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.1344621181488037, 'eval_F1': 0.7263142897848696, 'eval_precision': 0.9058280028429282, 'eval_recall': 0.6061831153388823, 'eval_runtime': 42.8382, 'eval_samples_per_second': 132.032, 'eval_steps_per_second': 8.264, 'epoch':1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:35<58:57,  2.10it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-2475\special_tokens_map.json
{'loss': 3.243, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.7667, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.738, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.5703, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.5574, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:30<40:06,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.743328332901001, 'eval_F1': 0.8450739850765145, 'eval_precision': 0.9024851431658563, 'eval_recall': 0.7945303210463733, 'eval_runtime': 42.869, 'eval_samples_per_second': 131.937, 'eval_steps_per_second': 8.258, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:13<40:06,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-4950\special_tokens_map.json
{'loss': 2.3752, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.5805, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.6984, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.6545, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.5915, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:04<19:49,  2.08it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.6060879230499268, 'eval_F1': 0.8586726435064126, 'eval_precision': 0.9012023000522739, 'eval_recall': 0.8199762187871581, 'eval_runtime': 42.7106, 'eval_samples_per_second': 132.426, 'eval_steps_per_second': 8.288, 'epoch':3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:47<19:49,  2.08it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-7425\special_tokens_map.json
{'loss': 1.5471, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 1.1413, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.1856, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.0869, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.114, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:29:36<00:00,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 5.399053573608398, 'eval_F1': 0.8560606060606061, 'eval_precision': 0.9125168236877523, 'eval_recall': 0.8061831153388823, 'eval_runtime': 42.8887, 'eval_samples_per_second': 131.876, 'eval_steps_per_second': 8.254, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:19<00:00,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold4\checkpoint-7425 (score: 0.8586726435064126).
{'train_runtime': 5422.464, 'train_samples_per_second': 29.206, 'train_steps_per_second': 1.826, 'train_loss': 2.264212816026476, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:22<00:00,  1.83it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.31it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.43it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.7394, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.4157, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.1538, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.0262, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [21:37<56:25,  2.19it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.7607600688934326, 'eval_F1': 0.8078141499472017, 'eval_precision': 0.9134328358208955, 'eval_recall': 0.7240889730241363, 'eval_runtime': 41.2303, 'eval_samples_per_second': 137.181, 'eval_steps_per_second': 8.586, 'epoch':1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:18<56:25,  2.19it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-2475\special_tokens_map.json
{'loss': 2.9532, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.1766, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.1357, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.1538, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.0492, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:07<40:07,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.0133583545684814, 'eval_F1': 0.8276584599266632, 'eval_precision': 0.9266862170087976, 'eval_recall': 0.7477520113582584, 'eval_runtime': 42.8669, 'eval_samples_per_second': 131.943, 'eval_steps_per_second': 8.258, 'epoch':2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:49<40:07,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-4950\special_tokens_map.json
{'loss': 1.9241, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.2534, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.2919, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.277, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.3036, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:06:46<19:12,  2.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.34583044052124, 'eval_F1': 0.841833440929632, 'eval_precision': 0.9263995453253765, 'eval_recall': 0.7714150496923805, 'eval_runtime': 42.6925, 'eval_samples_per_second': 132.482, 'eval_steps_per_second': 8.292, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:29<19:12,  2.15it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-7425\special_tokens_map.json
{'loss': 1.196, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 1.0011, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.0394, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.048, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.0811, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:29:33<00:00,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.359196662902832, 'eval_F1': 0.8645209580838323, 'eval_precision': 0.9142480211081794, 'eval_recall': 0.8199242782773308, 'eval_runtime': 42.756, 'eval_samples_per_second': 132.286, 'eval_steps_per_second': 8.28, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:16<00:00,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold5\checkpoint-9900 (score: 0.8645209580838323).
{'train_runtime': 5419.6253, 'train_samples_per_second': 29.221, 'train_steps_per_second': 1.827, 'train_loss': 1.9216860715307371, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:19<00:00,  1.83it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.36it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.48it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.8673, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.5076, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.237, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.1457, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [21:55<58:07,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.80702543258667, 'eval_F1': 0.798140770252324, 'eval_precision': 0.9147640791476408, 'eval_recall': 0.7078916372202592, 'eval_runtime': 42.6973, 'eval_samples_per_second': 132.467, 'eval_steps_per_second': 8.291, 'epoch': 1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:37<58:07,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-2475\special_tokens_map.json
{'loss': 2.9359, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.28, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.2371, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.2162, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.0838, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:42<39:33,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.7914555072784424, 'eval_F1': 0.8669769720725136, 'eval_precision': 0.9030364889002297, 'eval_recall': 0.8336866902237927, 'eval_runtime': 42.6715, 'eval_samples_per_second': 132.547, 'eval_steps_per_second': 8.296, 'epoch':2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:25<39:33,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-4950\special_tokens_map.json
{'loss': 2.0352, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3735, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.341, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.3811, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.4444, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:30<19:53,  2.07it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.685921669006348, 'eval_F1': 0.8482142857142858, 'eval_precision': 0.9248956884561892, 'eval_recall': 0.7832744405182568, 'eval_runtime': 42.6838, 'eval_samples_per_second': 132.509, 'eval_steps_per_second': 8.294, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:13<19:53,  2.07it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-7425\special_tokens_map.json
{'loss': 1.2459, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 1.0377, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.169, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.0466, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.1615, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:07<00:00,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.338578701019287, 'eval_F1': 0.8720216163104888, 'eval_precision': 0.9109571465229663, 'eval_recall': 0.8362779740871613, 'eval_runtime': 42.7589, 'eval_samples_per_second': 132.276, 'eval_steps_per_second': 8.279, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:50<00:00,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold6\checkpoint-9900 (score: 0.8720216163104888).
{'train_runtime': 5453.4257, 'train_samples_per_second': 29.04, 'train_steps_per_second': 1.815, 'train_loss': 1.999587778418955, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:53<00:00,  1.82it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.34it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.44it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache atC:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.jsonfrom cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 3.8571, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 3.5741, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 3.3063, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 3.1772, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████                                                         | 2475/9900 [22:08<1:01:27,  2.01it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.003938674926758, 'eval_F1': 0.7422916965438119, 'eval_precision': 0.9279311581211904, 'eval_recall': 0.6185468451242829, 'eval_runtime': 42.0348, 'eval_samples_per_second': 134.555, 'eval_steps_per_second': 8.422, 'epoch': 1.0}
 25%|███████████████████                                                         | 2475/9900 [22:50<1:01:27,  2.01it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-2475\special_tokens_map.json
{'loss': 3.0108, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 2.2768, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 2.2309, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 2.2753, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 2.1684, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [44:52<40:05,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.879185199737549, 'eval_F1': 0.831419624217119, 'eval_precision': 0.9155172413793103, 'eval_recall': 0.761472275334608, 'eval_runtime': 42.7039, 'eval_samples_per_second': 132.447, 'eval_steps_per_second': 8.29, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:35<40:05,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-4950\special_tokens_map.json
{'loss': 2.0557, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 1.3864, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 1.3569, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 1.4164, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 1.298, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:07:37<19:36,  2.10it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.6522436141967773, 'eval_F1': 0.8616305160807779, 'eval_precision': 0.9004689942678479, 'eval_recall': 0.8260038240917782, 'eval_runtime': 42.832, 'eval_samples_per_second': 132.051, 'eval_steps_per_second': 8.265, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:19<19:36,  2.10it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-7425\special_tokens_map.json
{'loss': 1.2863, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.9324, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 1.0826, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 1.0781, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 1.0935, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:36<00:00,  2.10it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 6.601251125335693, 'eval_F1': 0.8603281974195165, 'eval_precision': 0.9039220847591471, 'eval_recall': 0.8207456978967496, 'eval_runtime': 42.8746, 'eval_samples_per_second': 131.92, 'eval_steps_per_second': 8.257, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:19<00:00,  2.10it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold7\checkpoint-7425 (score: 0.8616305160807779).
{'train_runtime': 5482.6225, 'train_samples_per_second': 28.885, 'train_steps_per_second': 1.806, 'train_loss': 2.007938879764441, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:22<00:00,  1.81it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.25it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.36it/s]
(general-torch) PS D:\Develop\chinese-grammar-error-detection> 74363
