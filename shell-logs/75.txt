Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows

Loading personal and system profiles took 1257ms.
(base) PS D:\Develop\chinese-grammar-error-detection> conda activate general-torch
(general-torch) PS D:\Develop\chinese-grammar-error-detection> sh .\train-ner.sh
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5315, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.3068, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.0811, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0948, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|███████████████████▌                                                          | 2475/9900 [22:16<58:21,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.168829917907715, 'eval_F1': 0.8712984054669703, 'eval_precision': 0.8349705304518664, 'eval_recall': 0.9109311740890689, 'eval_runtime': 43.1153, 'eval_samples_per_second': 131.183, 'eval_steps_per_second': 8.211, 'epoch': 1.0}
 25%|███████████████████▌                                                          | 2475/9900 [22:59<58:21,  2.12it/s]                                                           Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-2475\special_tokens_map.json
{'loss': 1.9846, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.4736, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.4518, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.3771, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.4012, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:16<41:48,  1.97it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.3541524410247803, 'eval_F1': 0.781545302946081, 'eval_precision': 0.9382716049382716, 'eval_recall': 0.669683257918552, 'eval_runtime': 42.348, 'eval_samples_per_second': 133.56, 'eval_steps_per_second': 8.359, 'epoch': 2.0}
 50%|███████████████████████████████████████                                       | 4950/9900 [45:58<41:48,  1.97it/s]                                                           Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-4950\special_tokens_map.json
{'loss': 1.3178, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8294, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8409, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.8569, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8398, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:17<20:12,  2.04it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.1492254734039307, 'eval_F1': 0.8508483160293746, 'eval_precision': 0.9083536090835361, 'eval_recall': 0.8001905215527506, 'eval_runtime': 41.6129, 'eval_samples_per_second': 135.919, 'eval_steps_per_second': 8.507, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████                   | 7425/9900 [1:08:59<20:12,  2.04it/s]                                                           Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-7425\special_tokens_map.json
{'loss': 0.8115, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.6278, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.593, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.626, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.5971, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:07<00:00,  2.05it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.57700777053833, 'eval_F1': 0.8546965499874087, 'eval_precision': 0.9067592839967941, 'eval_recall': 0.8082876875446535, 'eval_runtime': 43.2179, 'eval_samples_per_second': 130.872, 'eval_steps_per_second': 8.191, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:50<00:00,  2.05it/s]                                                           Saving model checkpoint to ./ner_run_v2\fold0\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold0\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold0\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold0\checkpoint-2475 (score: 0.8712984054669703).
{'train_runtime': 5514.134, 'train_samples_per_second': 28.72, 'train_steps_per_second': 1.795, 'train_loss': 1.2681133308795967, 'epoch': 4.0}
100%|████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:54<00:00,  1.80it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:43<00:00,  8.19it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.48it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5603, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.301, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1428, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0813, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [21:56<58:12,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.8476651906967163, 'eval_F1': 0.8546637744034709, 'eval_precision': 0.8755555555555555, 'eval_recall': 0.8347457627118644, 'eval_runtime': 41.5566, 'eval_samples_per_second': 136.104, 'eval_steps_per_second': 8.519, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:37<58:12,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-2475\special_tokens_map.json
{'loss': 1.9694, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.5219, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.492, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.431, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.3624, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [44:41<37:54,  2.18it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9204702377319336, 'eval_F1': 0.8655214723926381, 'eval_precision': 0.9038954382368016, 'eval_recall': 0.8302730696798494, 'eval_runtime': 41.6141, 'eval_samples_per_second': 135.915, 'eval_steps_per_second': 8.507, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:23<37:54,  2.18it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-4950\special_tokens_map.json
{'loss': 1.3783, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8732, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8873, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.8827, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8701, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:07:40<20:23,  2.02it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.6019554138183594, 'eval_F1': 0.8900956203517885, 'eval_precision': 0.8927302865261663, 'eval_recall': 0.8874764595103578, 'eval_runtime': 41.4992, 'eval_samples_per_second': 136.292, 'eval_steps_per_second': 8.53, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:21<20:23,  2.02it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-7425\special_tokens_map.json
{'loss': 0.8668, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.6551, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.5958, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.6911, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.6466, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:45<00:00,  2.03it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.094297885894775, 'eval_F1': 0.8716491661519458, 'eval_precision': 0.917078242786587, 'eval_recall': 0.8305084745762712, 'eval_runtime': 43.7207, 'eval_samples_per_second': 129.367, 'eval_steps_per_second': 8.097, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:29<00:00,  2.03it/s]Saving model checkpoint to ./ner_run_v2\fold1\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold1\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold1\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold1\checkpoint-7425 (score: 0.8900956203517885).
{'train_runtime': 5493.0361, 'train_samples_per_second': 28.831, 'train_steps_per_second': 1.802, 'train_loss': 1.2933594659362178, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:32<00:00,  1.80it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.43it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.58it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5736, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.2975, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1666, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.086, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:05<58:31,  2.11it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9644780158996582, 'eval_F1': 0.7663369095722447, 'eval_precision': 0.93346911065852, 'eval_recall': 0.6499645473883243, 'eval_runtime': 41.539, 'eval_samples_per_second': 136.161, 'eval_steps_per_second': 8.522, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:47<58:31,  2.11it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-2475\special_tokens_map.json
{'loss': 1.9496, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.4587, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.4853, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.4223, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.4122, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:03<40:08,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.8729108572006226, 'eval_F1': 0.8460453787975901, 'eval_precision': 0.9243697478991597, 'eval_recall': 0.7799574568659892, 'eval_runtime': 41.9511, 'eval_samples_per_second': 134.824, 'eval_steps_per_second': 8.438, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:45<40:08,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-4950\special_tokens_map.json
{'loss': 1.3425, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8785, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.9398, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.8632, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8784, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:02<19:57,  2.07it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.6702611446380615, 'eval_F1': 0.8342512457382639, 'eval_precision': 0.9369661266568483, 'eval_recall': 0.7518317182699126, 'eval_runtime': 42.3517, 'eval_samples_per_second': 133.548, 'eval_steps_per_second': 8.359, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:44<19:57,  2.07it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-7425\special_tokens_map.json
{'loss': 0.839, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.5719, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.609, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.6717, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.6149, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:19<00:00,  2.07it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.213175296783447, 'eval_F1': 0.8682402897464718, 'eval_precision': 0.9205508474576272, 'eval_recall': 0.8215551878988419, 'eval_runtime': 41.7215, 'eval_samples_per_second': 135.566, 'eval_steps_per_second': 8.485, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:01<00:00,  2.07it/s]Saving model checkpoint to ./ner_run_v2\fold2\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold2\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold2\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold2\checkpoint-9900 (score: 0.8682402897464718).
{'train_runtime': 5524.693, 'train_samples_per_second': 28.665, 'train_steps_per_second': 1.792, 'train_loss': 1.2941393303148674, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:04<00:00,  1.79it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:41<00:00,  8.63it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.73it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5837, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.3708, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1679, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0457, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:17<59:01,  2.10it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.1466832160949707, 'eval_F1': 0.7528409090909091, 'eval_precision': 0.9344146685472496, 'eval_recall': 0.6303520456707897, 'eval_runtime': 41.8636, 'eval_samples_per_second': 135.106, 'eval_steps_per_second': 8.456, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:59<59:01,  2.10it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-2475\special_tokens_map.json
{'loss': 1.9391, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.473, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.5079, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.3956, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.3823, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:18<39:25,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.0216519832611084, 'eval_F1': 0.8603811184456345, 'eval_precision': 0.9030065359477124, 'eval_recall': 0.8215984776403426, 'eval_runtime': 43.3995, 'eval_samples_per_second': 130.324, 'eval_steps_per_second': 8.157, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [46:02<39:25,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-4950\special_tokens_map.json
{'loss': 1.3001, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8697, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8725, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.8442, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.9, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:23<19:59,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 3.2272393703460693, 'eval_F1': 0.878078369525658, 'eval_precision': 0.8960138648180243, 'eval_recall': 0.8608468125594672, 'eval_runtime': 43.3208, 'eval_samples_per_second': 130.561, 'eval_steps_per_second': 8.172, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:09:07<19:59,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-7425\special_tokens_map.json
{'loss': 0.8135, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.6058, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.6796, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.5799, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.5829, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:54:13<00:00,  2.24it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.456099033355713, 'eval_F1': 0.8673647469458987, 'eval_precision': 0.9112100576217915, 'eval_recall': 0.8275451950523312, 'eval_runtime': 40.022, 'eval_samples_per_second': 141.322, 'eval_steps_per_second': 8.845, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:54:54<00:00,  2.24it/s]Saving model checkpoint to ./ner_run_v2\fold3\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold3\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold3\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold3\checkpoint-7425 (score: 0.878078369525658).
{'train_runtime': 6897.5681, 'train_samples_per_second': 22.96, 'train_steps_per_second': 1.435, 'train_loss': 1.28238971440479, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:54:57<00:00,  1.44it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:40<00:00,  8.68it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.72it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5404, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.3472, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1876, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0701, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:12<57:57,  2.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.8923379182815552, 'eval_F1': 0.7790344827586206, 'eval_precision': 0.9168831168831169, 'eval_recall': 0.6772182254196643, 'eval_runtime': 41.8528, 'eval_samples_per_second': 135.14, 'eval_steps_per_second': 8.458, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:54<57:57,  2.14it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-2475\special_tokens_map.json
{'loss': 1.9222, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.4698, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.4442, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.3884, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.3846, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [44:43<38:17,  2.15it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.8858190774917603, 'eval_F1': 0.8736223365172667, 'eval_precision': 0.8926426426426426, 'eval_recall': 0.8553956834532375, 'eval_runtime': 41.7448, 'eval_samples_per_second': 135.49, 'eval_steps_per_second': 8.48, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:25<38:17,  2.15it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-4950\special_tokens_map.json
{'loss': 1.3462, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8348, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8629, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.807, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.9487, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:07:27<19:19,  2.13it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.7151904106140137, 'eval_F1': 0.8626401630988787, 'eval_precision': 0.9203371397498641, 'eval_recall': 0.8117505995203836, 'eval_runtime': 41.8861, 'eval_samples_per_second': 135.033, 'eval_steps_per_second': 8.451, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:09<19:19,  2.13it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-7425\special_tokens_map.json
{'loss': 0.8372, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.579, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.5954, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.6078, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.624, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:08<00:00,  2.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.405413627624512, 'eval_F1': 0.8714285714285714, 'eval_precision': 0.9125984251968504, 'eval_recall': 0.8338129496402877, 'eval_runtime': 41.7121, 'eval_samples_per_second': 135.596, 'eval_steps_per_second': 8.487, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:50<00:00,  2.14it/s]Saving model checkpoint to ./ner_run_v2\fold4\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold4\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold4\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold4\checkpoint-4950 (score: 0.8736223365172667).
{'train_runtime': 5454.1537, 'train_samples_per_second': 29.036, 'train_steps_per_second': 1.815, 'train_loss': 1.2765939855093908, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:54<00:00,  1.82it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:40<00:00,  8.64it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.77it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5056, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.2701, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1846, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0166, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:10<57:52,  2.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.869277000427246, 'eval_F1': 0.8702791461412152, 'eval_precision': 0.8633930649290202, 'eval_recall': 0.8772759517616457, 'eval_runtime': 41.7253, 'eval_samples_per_second': 135.553, 'eval_steps_per_second': 8.484, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:52<57:52,  2.14it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-2475\special_tokens_map.json
{'loss': 1.9178, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.4723, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.437, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.3266, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.4, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:05<40:43,  2.03it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.0468196868896484, 'eval_F1': 0.8739045764362219, 'eval_precision': 0.9004263857536995, 'eval_recall': 0.8489004492787893, 'eval_runtime': 41.606, 'eval_samples_per_second': 135.942, 'eval_steps_per_second': 8.508, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:47<40:43,  2.03it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-4950\special_tokens_map.json
{'loss': 1.2924, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.836, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.9066, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.8868, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8222, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:01<19:42,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.8967692852020264, 'eval_F1': 0.8637724550898203, 'eval_precision': 0.9141800897808291, 'eval_recall': 0.818633246630409, 'eval_runtime': 41.4505, 'eval_samples_per_second': 136.452, 'eval_steps_per_second': 8.54, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:43<19:42,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-7425\special_tokens_map.json
{'loss': 0.7849, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.608, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.705, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.5248, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.6183, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:30:55<00:00,  2.08it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.3830952644348145, 'eval_F1': 0.8749080206033849, 'eval_precision': 0.9087898089171974, 'eval_recall': 0.8434618113029085, 'eval_runtime': 43.5216, 'eval_samples_per_second': 129.958, 'eval_steps_per_second': 8.134, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:39<00:00,  2.08it/s]Saving model checkpoint to ./ner_run_v2\fold5\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold5\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold5\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold5\checkpoint-9900 (score: 0.8749080206033849).
{'train_runtime': 5503.0289, 'train_samples_per_second': 28.778, 'train_steps_per_second': 1.799, 'train_loss': 1.2608964122425426, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:42<00:00,  1.80it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:43<00:00,  8.23it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.34it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5444, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.3536, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1555, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.0437, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|█████████████████████████████████▊                                                                                                     | 2475/9900 [22:23<1:02:34,  1.98it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9521799087524414, 'eval_F1': 0.7305406580663865, 'eval_precision': 0.9470124013528749, 'eval_recall': 0.5946201038225578, 'eval_runtime': 42.3789, 'eval_samples_per_second': 133.462, 'eval_steps_per_second': 8.353, 'epoch': 1.0}
 25%|█████████████████████████████████▊                                                                                                     | 2475/9900 [23:05<1:02:34,  1.98it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-2475\special_tokens_map.json
{'loss': 1.9618, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.4323, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.4339, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.4253, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.3727, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:36<40:44,  2.02it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9222010374069214, 'eval_F1': 0.8261898501183277, 'eval_precision': 0.9328978622327792, 'eval_recall': 0.7413874469089193, 'eval_runtime': 41.6397, 'eval_samples_per_second': 135.832, 'eval_steps_per_second': 8.502, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [46:17<40:44,  2.02it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-4950\special_tokens_map.json
{'loss': 1.3024, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.857, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8559, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.85, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8702, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:58<19:27,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.7430381774902344, 'eval_F1': 0.8638920134983127, 'eval_precision': 0.9184161573212862, 'eval_recall': 0.8154789995280792, 'eval_runtime': 41.6784, 'eval_samples_per_second': 135.706, 'eval_steps_per_second': 8.494, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:09:39<19:27,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-7425\special_tokens_map.json
{'loss': 0.7955, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.5508, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.6349, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.639, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.5755, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:59<00:00,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.38116455078125, 'eval_F1': 0.8747695710949981, 'eval_precision': 0.9127981533726597, 'eval_recall': 0.839782916470033, 'eval_runtime': 43.8187, 'eval_samples_per_second': 129.077, 'eval_steps_per_second': 8.079, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:43<00:00,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold6\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold6\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold6\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold6\checkpoint-9900 (score: 0.8747695710949981).
{'train_runtime': 5567.0641, 'train_samples_per_second': 28.447, 'train_steps_per_second': 1.778, 'train_loss': 1.2684308262064, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:47<00:00,  1.78it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:42<00:00,  8.29it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.34it/s]
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\229ce68be3619b81ba7498cc40301675d5d923a5c43f20e1db15d4b15f5c2006.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9dff099b9b46efd82fbc99787839c808ef5a7aa3dde3ed8fe0bfd8416b81e627.660ed5c7513bf13d4607410502a84e0de517eb889ff8c401068a1688868e1ccb
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/added_tokens.json from cache at C:\Users\holaj/.cache\huggingface\transformers\6ee5a7f5e3aa2ba5afa7127d6be5c3451dbaf5d467557686094617bdf2beab74.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\9f382d012cf0761684535742ec8cf1096e0be52359ee0389a418eaada1265367.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\28fd86fdb44cd30e24e9793d571ae709f73ce1aa12fb0d3e60e826d7c250a5a8.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "_name_or_path": "hfl/chinese-macbert-base",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/vocab.txt from cache at C:\Users\holaj/.cache\huggingface\transformers\937ed6bd86ba6e74ad564aed894f5961e920fdeed31b6259b220219076dc0402.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/special_tokens_map.json from cache at C:\Users\holaj/.cache\huggingface\transformers\aabf80a224562bad14cbf011d94cb02f690c14acf6a84d41b0a0bc1d30078d43.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/tokenizer_config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\36ff5d69dc119ded1838ce6c29076b609a97257597137c2f1cff3282b8b8320d.79ba9d36d90a51c686e2727c4a0619b63346fd3a9f715a52bb46ee296ed21d99
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "_name_or_path": "uer/roberta-base-finetuned-cluener2020-chinese",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
D:\Develop\chinese-grammar-error-detection\dataset.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  indexed_value = torch.tensor(value[index]).squeeze()
loading configuration file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\dfbc5cae1d1506a97fa3e0014d34afdd480ea8cce63302d8df982725a3e5b77a.ddc6437c26ae4493a14d93f62bb62ae67d1e46c0de74ff95524ad468354a19f7
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/hfl/chinese-macbert-base/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\f350d12c99d2a8d00f4299b8e292c2248422676424702a2c45a8a3d65646f738.749c1a543002a65141e104ba5e040263fd8eabc9d2dcfb537bf681345565ef45
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at hfl/chinese-macbert-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading configuration file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/config.json from cache at C:\Users\holaj/.cache\huggingface\transformers\c2bcf3c2fd0a437e0fa90492f3753d8652b9700190ec65ef4100b6d8611b47e0.5e0ccf4386f860080f9bf051c79a544cc36dbe645be37c536bcb712859545818
Model config BertConfig {
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "O",
    "1": "B-address",
    "2": "I-address",
    "3": "B-book",
    "4": "I-book",
    "5": "B-company",
    "6": "I-company",
    "7": "B-game",
    "8": "I-game",
    "9": "B-government",
    "10": "I-government",
    "11": "B-movie",
    "12": "I-movie",
    "13": "B-name",
    "14": "I-name",
    "15": "B-organization",
    "16": "I-organization",
    "17": "B-position",
    "18": "I-position",
    "19": "B-scene",
    "20": "I-scene",
    "21": "S-address",
    "22": "S-book",
    "23": "S-company",
    "24": "S-game",
    "25": "S-government",
    "26": "S-movie",
    "27": "S-name",
    "28": "S-organization",
    "29": "S-position",
    "30": "S-scene",
    "31": "[PAD]"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-address": 1,
    "B-book": 3,
    "B-company": 5,
    "B-game": 7,
    "B-government": 9,
    "B-movie": 11,
    "B-name": 13,
    "B-organization": 15,
    "B-position": 17,
    "B-scene": 19,
    "I-address": 2,
    "I-book": 4,
    "I-company": 6,
    "I-game": 8,
    "I-government": 10,
    "I-movie": 12,
    "I-name": 14,
    "I-organization": 16,
    "I-position": 18,
    "I-scene": 20,
    "O": 0,
    "S-address": 21,
    "S-book": 22,
    "S-company": 23,
    "S-game": 24,
    "S-government": 25,
    "S-movie": 26,
    "S-name": 27,
    "S-organization": 28,
    "S-position": 29,
    "S-scene": 30,
    "[PAD]": 31
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading weights file https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese/resolve/main/pytorch_model.bin from cache at C:\Users\holaj/.cache\huggingface\transformers\caf4d6669137dc5f2e41e83348de76fef9ca0df8ebead544ef8e7bea736c0daf.7db7a67184b90ba9ee90458ce839792cf8fc9a12d76dcfb2e291cb4262cfebf8
Some weights of the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at uer/roberta-base-finetuned-cluener2020-chinese and are newly initialized: ['bert.pooler.dense.weight','bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
D:\Apps\Anaconda3\envs\general-torch\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a futureversion. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39592
  Num Epochs = 4
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 9900
{'loss': 2.5751, 'learning_rate': 1.8989898989898993e-05, 'epoch': 0.2}
{'loss': 2.318, 'learning_rate': 1.797979797979798e-05, 'epoch': 0.4}
{'loss': 2.1953, 'learning_rate': 1.6969696969696972e-05, 'epoch': 0.61}
{'loss': 2.1202, 'learning_rate': 1.595959595959596e-05, 'epoch': 0.81}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [22:21<57:56,  2.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9887737035751343, 'eval_F1': 0.8531988956907934, 'eval_precision': 0.8689486552567237, 'eval_recall': 0.8380099033246876, 'eval_runtime': 41.7932, 'eval_samples_per_second': 135.333, 'eval_steps_per_second': 8.47, 'epoch': 1.0}
 25%|██████████████████████████████████▎                                                                                                      | 2475/9900 [23:03<57:56,  2.14it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-2475
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-2475\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-2475\special_tokens_map.json
{'loss': 1.9948, 'learning_rate': 1.4949494949494952e-05, 'epoch': 1.01}
{'loss': 1.5093, 'learning_rate': 1.3939393939393942e-05, 'epoch': 1.21}
{'loss': 1.5032, 'learning_rate': 1.2929292929292931e-05, 'epoch': 1.41}
{'loss': 1.4667, 'learning_rate': 1.191919191919192e-05, 'epoch': 1.62}
{'loss': 1.4484, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.82}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [45:20<39:00,  2.12it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 1.9089281558990479, 'eval_F1': 0.8555085799552351, 'eval_precision': 0.9050249934227835, 'eval_recall': 0.8111294506012733, 'eval_runtime': 43.224, 'eval_samples_per_second': 130.853, 'eval_steps_per_second': 8.19, 'epoch': 2.0}
 50%|████████████████████████████████████████████████████████████████████▌                                                                    | 4950/9900 [46:03<39:00,  2.12it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-4950
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-4950\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-4950\special_tokens_map.json
{'loss': 1.3906, 'learning_rate': 9.8989898989899e-06, 'epoch': 2.02}
{'loss': 0.8468, 'learning_rate': 8.888888888888888e-06, 'epoch': 2.22}
{'loss': 0.8722, 'learning_rate': 7.87878787878788e-06, 'epoch': 2.42}
{'loss': 0.9248, 'learning_rate': 6.868686868686869e-06, 'epoch': 2.63}
{'loss': 0.8598, 'learning_rate': 5.858585858585859e-06, 'epoch': 2.83}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:08:22<19:42,  2.09it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 2.799159288406372, 'eval_F1': 0.8594709790648114, 'eval_precision': 0.917558886509636, 'eval_recall': 0.8082999292619665, 'eval_runtime': 43.2529, 'eval_samples_per_second': 130.766, 'eval_steps_per_second': 8.184, 'epoch': 3.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 7425/9900 [1:09:05<19:42,  2.09it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-7425
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-7425\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-7425\special_tokens_map.json
{'loss': 0.8229, 'learning_rate': 4.848484848484849e-06, 'epoch': 3.03}
{'loss': 0.6481, 'learning_rate': 3.8383838383838385e-06, 'epoch': 3.23}
{'loss': 0.6426, 'learning_rate': 2.8282828282828286e-06, 'epoch': 3.43}
{'loss': 0.5817, 'learning_rate': 1.8181818181818183e-06, 'epoch': 3.64}
{'loss': 0.6482, 'learning_rate': 8.080808080808082e-07, 'epoch': 3.84}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:31:26<00:00,  2.06it/s]The following columns in the evaluation set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 5656
  Batch size = 16
{'eval_loss': 4.3069682121276855, 'eval_F1': 0.871177698636866, 'eval_precision': 0.9090210148641722, 'eval_recall': 0.8363593492100919, 'eval_runtime': 43.2984, 'eval_samples_per_second': 130.628, 'eval_steps_per_second': 8.176, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:09<00:00,  2.06it/s]Saving model checkpoint to ./ner_run_v2\fold7\checkpoint-9900
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
tokenizer config file saved in ./ner_run_v2\fold7\checkpoint-9900\tokenizer_config.json
Special tokens file saved in ./ner_run_v2\fold7\checkpoint-9900\special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./ner_run_v2\fold7\checkpoint-9900 (score: 0.871177698636866).
{'train_runtime': 5532.6017, 'train_samples_per_second': 28.625, 'train_steps_per_second': 1.789, 'train_loss': 1.3060730489095052, 'epoch': 4.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9900/9900 [1:32:12<00:00,  1.79it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 5656
  Batch size = 16
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:43<00:00,  8.22it/s]
The following columns in the test set don't have a corresponding argument in `BertWithNER.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertWithNER.forward`,  you can safely ignore this message.
***** Running Prediction *****
  Num examples = 1039
  Batch size = 16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:07<00:00,  8.32it/s]
(general-torch) PS D:\Develop\chinese-grammar-error-detection>
